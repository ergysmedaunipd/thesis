{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUl/zT6eOP5YiW2twfwayd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ergysmedaunipd/thesis/blob/main/ThesisUnipdSNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tonic\n",
        "!pip install snntorch\n",
        "!pip install psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc3-4tqnv5AS",
        "outputId": "dd6400b9-c02b-4924-b507-90ada7590a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tonic\n",
            "  Downloading tonic-1.5.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from tonic) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from tonic) (3.12.1)\n",
            "Collecting importRosbag>=1.0.4 (from tonic)\n",
            "  Downloading importRosbag-1.0.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tonic) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tonic) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from tonic) (4.12.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from tonic) (0.10.2.post1)\n",
            "Collecting pbr (from tonic)\n",
            "  Downloading pbr-6.1.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting expelliarmus (from tonic)\n",
            "  Downloading expelliarmus-1.1.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from importRosbag>=1.0.4->tonic) (75.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa->tonic) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->tonic) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->tonic) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->tonic) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->tonic) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->tonic) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tonic) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (2024.8.30)\n",
            "Downloading tonic-1.5.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importRosbag-1.0.4-py3-none-any.whl (28 kB)\n",
            "Downloading expelliarmus-1.1.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pbr, importRosbag, expelliarmus, tonic\n",
            "Successfully installed expelliarmus-1.1.12 importRosbag-1.0.4 pbr-6.1.0 tonic-1.5.0\n",
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.26.4)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-1.0.4-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.12.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (3.0.2)\n",
            "Downloading snntorch-0.9.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nir-1.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: nir, nirtorch, snntorch\n",
            "Successfully installed nir-1.0.4 nirtorch-1.0 snntorch-0.9.1\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcOCVUH7r4kE"
      },
      "outputs": [],
      "source": [
        "def prepare_nmnist_data(inputs, labels, device, n_timesteps=100):\n",
        "    \"\"\"\n",
        "    Modified data preparation with consistent device placement.\n",
        "    \"\"\"\n",
        "    batch_size = inputs.shape[0]\n",
        "\n",
        "    # Move inputs to device first\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "    # Reshape inputs to [time, features, batch]\n",
        "    inputs = inputs.float().reshape(batch_size, -1, sensor_size[0] * sensor_size[1])\n",
        "    inputs = inputs.permute(1, 2, 0).contiguous()\n",
        "\n",
        "    # Handle timesteps\n",
        "    if inputs.shape[0] > n_timesteps:\n",
        "        inputs = inputs[:n_timesteps]\n",
        "    elif inputs.shape[0] < n_timesteps:\n",
        "        padding = torch.zeros((n_timesteps - inputs.shape[0], inputs.shape[1], batch_size),\n",
        "                            device=device)\n",
        "        inputs = torch.cat((inputs, padding), dim=0)\n",
        "\n",
        "    # One-hot encode labels (now labels are already on correct device)\n",
        "    labels_onehot = torch.zeros((10, batch_size), device=device)\n",
        "    labels_onehot.scatter_(0, labels.unsqueeze(0), 1)\n",
        "\n",
        "    return inputs, labels_onehot\n",
        "\n",
        "def train(model, trainloader, num_epochs):\n",
        "\n",
        "    # Warming phase\n",
        "    print(\"\\nWarming Phase:\")\n",
        "    warming_losses = []\n",
        "    for epoch in range(5):\n",
        "        print(f'Warming Epoch [{epoch+1}/2]')\n",
        "        epoch_losses = []\n",
        "        epoch_accuracies = []\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Prepare data\n",
        "            inputs, labels = prepare_nmnist_data(inputs, labels, device)\n",
        "\n",
        "            # Warming step\n",
        "            loss, predictions = model.warming(inputs, labels)\n",
        "\n",
        "            # Ensure predictions and labels have correct shape\n",
        "            # predictions: [batch_size, num_classes]\n",
        "            # labels: [num_classes, batch_size] -> [batch_size, num_classes]\n",
        "            labels = labels.t()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_classes = torch.argmax(predictions, dim=1)  # [batch_size]\n",
        "            true_classes = torch.argmax(labels, dim=1)      # [batch_size]\n",
        "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
        "\n",
        "            epoch_losses.append(loss)\n",
        "            epoch_accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "        # Epoch summary\n",
        "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        avg_acc = sum(epoch_accuracies) / len(epoch_accuracies)\n",
        "        warming_losses.append(avg_loss)\n",
        "        print(f'  Epoch Summary - Loss: {avg_loss:.6f}, Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    # Main training\n",
        "    print(\"\\nMain Training Phase:\")\n",
        "    training_metrics = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        epoch_losses = []\n",
        "        epoch_accuracies = []\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "            print(f'  Batch [{batch_idx+1}/{len(trainloader)}]')\n",
        "\n",
        "            # Prepare data\n",
        "            inputs, labels = prepare_nmnist_data(inputs, labels, device)\n",
        "\n",
        "            # Training step\n",
        "            loss, predictions = model.fit(inputs, labels)\n",
        "\n",
        "            # Ensure predictions and labels have correct shape\n",
        "            labels = labels.t()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_classes = torch.argmax(predictions, dim=1)\n",
        "            true_classes = torch.argmax(labels, dim=1)\n",
        "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
        "\n",
        "            epoch_losses.append(loss)\n",
        "            epoch_accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "        # Epoch summary\n",
        "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        avg_acc = sum(epoch_accuracies) / len(epoch_accuracies)\n",
        "        training_metrics.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'loss': avg_loss,\n",
        "            'accuracy': avg_acc\n",
        "        })\n",
        "        print(f'  Epoch Summary - Loss: {avg_loss:.6f}, Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    return warming_losses, training_metrics\n",
        "\n",
        "def evaluate(model, testloader):\n",
        "    print(\"\\nEvaluation Phase:\")\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    num_batches = 0\n",
        "    batch_metrics = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(testloader):\n",
        "            print(f'  Batch [{batch_idx+1}/{len(testloader)}]')\n",
        "\n",
        "            # Prepare data\n",
        "            inputs, labels = prepare_nmnist_data(inputs, labels, device)\n",
        "\n",
        "            # Forward pass\n",
        "            loss, predictions = model.evaluate(inputs, labels)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_classes = torch.argmax(predictions, dim=0)\n",
        "            true_classes = torch.argmax(labels, dim=0)\n",
        "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
        "\n",
        "            # Store batch metrics\n",
        "            batch_metrics.append({\n",
        "                'batch': batch_idx + 1,\n",
        "                'loss': loss,\n",
        "                'accuracy': accuracy\n",
        "            })\n",
        "\n",
        "            total_loss += loss\n",
        "            total_acc += accuracy\n",
        "            num_batches += 1\n",
        "\n",
        "            print(f'    Loss: {loss:.6f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_acc = total_acc / num_batches\n",
        "\n",
        "    print(f'\\nFinal Evaluation Results:')\n",
        "    print(f'  Average Loss: {avg_loss:.6f}')\n",
        "    print(f'  Average Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    return avg_loss, avg_acc, batch_metrics\n",
        "\n",
        "# Initialize model and training\n",
        "n_timesteps = 100\n",
        "input_dim = sensor_size[0] * sensor_size[1]\n",
        "hidden_dims = [128, 10]\n",
        "n_outputs = 10\n",
        "# DataLoaders\n",
        "\n",
        "\n",
        "# Calculate size of 20% of data\n",
        "train_size = int(0.5 * len(trainset)) // 256 * 256\n",
        "test_size = int(0.5 * len(testset)) // 256 * 256\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "train_subset = Subset(trainset, torch.randperm(len(trainset))[:train_size])\n",
        "test_subset = Subset(testset, torch.randperm(len(testset))[:test_size])\n",
        "\n",
        "\n",
        "# Create DataLoaders with the subsets\n",
        "batch_size = 256\n",
        "trainloader = DataLoader(train_subset,\n",
        "                        batch_size=batch_size,\n",
        "                        collate_fn=tonic.collation.PadTensors(),\n",
        "                        shuffle=True)\n",
        "testloader = DataLoader(test_subset,\n",
        "                       batch_size=batch_size,\n",
        "                       collate_fn=tonic.collation.PadTensors())\n",
        "\n",
        "print(f\"Original training set size: {len(trainset)}\")\n",
        "print(f\"training set size: {len(train_subset)}\")\n",
        "print(f\"Original test set size: {len(testset)}\")\n",
        "print(f\"test set size: {len(test_subset)}\")\n",
        "\n",
        "# Hyperparameters\n",
        "rho = 0.05\n",
        "delta = torch.tensor(0.7, device=device)\n",
        "theta = torch.tensor(0.15, device=device)\n",
        "\n",
        "# Create model\n",
        "print(\"\\nInitializing model...\")\n",
        "model = ADMM_SNN(\n",
        "    n_samples=batch_size,\n",
        "    n_timesteps=n_timesteps,\n",
        "    input_dim=input_dim,\n",
        "    hidden_dims=hidden_dims,  # Changed final dimension to 10\n",
        "    n_outputs=10,\n",
        "    rho=rho,\n",
        "    delta=delta,\n",
        "    theta=theta\n",
        ")\n",
        "print(model)\n",
        "\n",
        "# Train model\n",
        "print(\"\\nStarting training process...\")\n",
        "num_epochs = 20\n",
        "warming_losses, training_metrics = train(model, trainloader, num_epochs)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_loss, test_acc, test_metrics = evaluate(model, testloader)\n",
        "\n",
        "# Print final results\n",
        "print(\"\\nTraining Summary:\")\n",
        "print(f\"  Warming phase final loss: {warming_losses[-1]:.6f}\")\n",
        "print(f\"  Training final loss: {training_metrics[-1]['loss']:.6f}\")\n",
        "print(f\"  Training final accuracy: {training_metrics[-1]['accuracy']:.4f}\")\n",
        "print(\"\\nTest Results:\")\n",
        "print(f\"  Test Loss: {test_loss:.6f}\")\n",
        "print(f\"  Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import tonic\n",
        "import tonic.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tonic import DiskCachedDataset\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations\n",
        "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
        "frame_transform = transforms.Compose([\n",
        "    transforms.Denoise(filter_time=10000),\n",
        "    transforms.ToFrame(sensor_size=sensor_size, time_window=1000)\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
        "testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYuF2XdhGXZg",
        "outputId": "5a36d8f8-4d58-4110-ed23-99bdc23b7077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes in __init__:\n",
            "W[0] shape: torch.Size([100, 1156])\n",
            "W[1] shape: torch.Size([50, 100])\n",
            "z[0] shape: torch.Size([300, 128, 100])\n",
            "z[1] shape: torch.Size([300, 128, 50])\n",
            "a[0] shape: torch.Size([300, 128, 100])\n",
            "a[1] shape: torch.Size([300, 128, 50])\n",
            "Sample data shape: torch.Size([128, 309, 2, 34, 34])\n",
            "Sample target shape: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_nmnist_data(inputs, labels, device, n_timesteps=100):\n",
        "    \"\"\"\n",
        "    Modified data preparation with consistent device placement.\n",
        "    \"\"\"\n",
        "    batch_size = inputs.shape[0]\n",
        "\n",
        "    # Move inputs to device first\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "    # Reshape inputs to [time, features, batch]\n",
        "    inputs = inputs.float().reshape(batch_size, -1, sensor_size[0] * sensor_size[1])\n",
        "    inputs = inputs.permute(1, 2, 0).contiguous()\n",
        "\n",
        "    # Handle timesteps\n",
        "    if inputs.shape[0] > n_timesteps:\n",
        "        inputs = inputs[:n_timesteps]\n",
        "    elif inputs.shape[0] < n_timesteps:\n",
        "        padding = torch.zeros((n_timesteps - inputs.shape[0], inputs.shape[1], batch_size),\n",
        "                            device=device)\n",
        "        inputs = torch.cat((inputs, padding), dim=0)\n",
        "\n",
        "    # One-hot encode labels (now labels are already on correct device)\n",
        "    labels_onehot = torch.zeros((10, batch_size), device=device)\n",
        "    labels_onehot.scatter_(0, labels.unsqueeze(0), 1)\n",
        "\n",
        "    return inputs, labels_onehot\n",
        "\n",
        "def train(model, trainloader, num_epochs):\n",
        "\n",
        "    # Warming phase\n",
        "    print(\"\\nWarming Phase:\")\n",
        "    warming_losses = []\n",
        "    for epoch in range(5):\n",
        "        print(f'Warming Epoch [{epoch+1}/2]')\n",
        "        epoch_losses = []\n",
        "        epoch_accuracies = []\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Prepare data\n",
        "            inputs, labels = prepare_nmnist_data(inputs, labels, device)\n",
        "\n",
        "            # Warming step\n",
        "            loss, predictions = model.warming(inputs, labels)\n",
        "\n",
        "            # Ensure predictions and labels have correct shape\n",
        "            # predictions: [batch_size, num_classes]\n",
        "            # labels: [num_classes, batch_size] -> [batch_size, num_classes]\n",
        "            labels = labels.t()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_classes = torch.argmax(predictions, dim=1)  # [batch_size]\n",
        "            true_classes = torch.argmax(labels, dim=1)      # [batch_size]\n",
        "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
        "\n",
        "            epoch_losses.append(loss)\n",
        "            epoch_accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "        # Epoch summary\n",
        "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        avg_acc = sum(epoch_accuracies) / len(epoch_accuracies)\n",
        "        warming_losses.append(avg_loss)\n",
        "        print(f'  Epoch Summary - Loss: {avg_loss:.6f}, Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    # Main training\n",
        "    print(\"\\nMain Training Phase:\")\n",
        "    training_metrics = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        epoch_losses = []\n",
        "        epoch_accuracies = []\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "            print(f'  Batch [{batch_idx+1}/{len(trainloader)}]')\n",
        "\n",
        "            # Prepare data\n",
        "            inputs, labels = prepare_nmnist_data(inputs, labels, device)\n",
        "\n",
        "            # Training step\n",
        "            loss, predictions = model.fit(inputs, labels)\n",
        "\n",
        "            # Ensure predictions and labels have correct shape\n",
        "            labels = labels.t()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_classes = torch.argmax(predictions, dim=1)\n",
        "            true_classes = torch.argmax(labels, dim=1)\n",
        "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
        "\n",
        "            epoch_losses.append(loss)\n",
        "            epoch_accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "        # Epoch summary\n",
        "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        avg_acc = sum(epoch_accuracies) / len(epoch_accuracies)\n",
        "        training_metrics.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'loss': avg_loss,\n",
        "            'accuracy': avg_acc\n",
        "        })\n",
        "        print(f'  Epoch Summary - Loss: {avg_loss:.6f}, Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    return warming_losses, training_metrics\n",
        "\n",
        "def evaluate(model, testloader):\n",
        "    print(\"\\nEvaluation Phase:\")\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    num_batches = 0\n",
        "    batch_metrics = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(testloader):\n",
        "            print(f'  Batch [{batch_idx+1}/{len(testloader)}]')\n",
        "\n",
        "            # Prepare data\n",
        "            inputs, labels = prepare_nmnist_data(inputs, labels, device)\n",
        "\n",
        "            # Forward pass\n",
        "            loss, predictions = model.evaluate(inputs, labels)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_classes = torch.argmax(predictions, dim=0)\n",
        "            true_classes = torch.argmax(labels, dim=0)\n",
        "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
        "\n",
        "            # Store batch metrics\n",
        "            batch_metrics.append({\n",
        "                'batch': batch_idx + 1,\n",
        "                'loss': loss,\n",
        "                'accuracy': accuracy\n",
        "            })\n",
        "\n",
        "            total_loss += loss\n",
        "            total_acc += accuracy\n",
        "            num_batches += 1\n",
        "\n",
        "            print(f'    Loss: {loss:.6f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_acc = total_acc / num_batches\n",
        "\n",
        "    print(f'\\nFinal Evaluation Results:')\n",
        "    print(f'  Average Loss: {avg_loss:.6f}')\n",
        "    print(f'  Average Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    return avg_loss, avg_acc, batch_metrics\n",
        "\n",
        "# Initialize model and training\n",
        "n_timesteps = 100\n",
        "input_dim = sensor_size[0] * sensor_size[1]\n",
        "hidden_dims = [128, 10]\n",
        "n_outputs = 10\n",
        "# DataLoaders\n",
        "\n",
        "\n",
        "# Calculate size of 20% of data\n",
        "train_size = int(0.5 * len(trainset)) // 256 * 256\n",
        "test_size = int(0.5 * len(testset)) // 256 * 256\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "train_subset = Subset(trainset, torch.randperm(len(trainset))[:train_size])\n",
        "test_subset = Subset(testset, torch.randperm(len(testset))[:test_size])\n",
        "\n",
        "\n",
        "# Create DataLoaders with the subsets\n",
        "batch_size = 256\n",
        "trainloader = DataLoader(train_subset,\n",
        "                        batch_size=batch_size,\n",
        "                        collate_fn=tonic.collation.PadTensors(),\n",
        "                        shuffle=True)\n",
        "testloader = DataLoader(test_subset,\n",
        "                       batch_size=batch_size,\n",
        "                       collate_fn=tonic.collation.PadTensors())\n",
        "\n",
        "print(f\"Original training set size: {len(trainset)}\")\n",
        "print(f\"training set size: {len(train_subset)}\")\n",
        "print(f\"Original test set size: {len(testset)}\")\n",
        "print(f\"test set size: {len(test_subset)}\")\n",
        "\n",
        "# Hyperparameters\n",
        "rho = 0.05\n",
        "delta = torch.tensor(0.7, device=device)\n",
        "theta = torch.tensor(0.15, device=device)\n",
        "\n",
        "# Create model\n",
        "print(\"\\nInitializing model...\")\n",
        "model = ADMM_SNN(\n",
        "    n_samples=batch_size,\n",
        "    n_timesteps=n_timesteps,\n",
        "    input_dim=input_dim,\n",
        "    hidden_dims=hidden_dims,  # Changed final dimension to 10\n",
        "    n_outputs=10,\n",
        "    rho=rho,\n",
        "    delta=delta,\n",
        "    theta=theta\n",
        ")\n",
        "print(model)\n",
        "\n",
        "# Train model\n",
        "print(\"\\nStarting training process...\")\n",
        "num_epochs = 20\n",
        "warming_losses, training_metrics = train(model, trainloader, num_epochs)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_loss, test_acc, test_metrics = evaluate(model, testloader)\n",
        "\n",
        "# Print final results\n",
        "print(\"\\nTraining Summary:\")\n",
        "print(f\"  Warming phase final loss: {warming_losses[-1]:.6f}\")\n",
        "print(f\"  Training final loss: {training_metrics[-1]['loss']:.6f}\")\n",
        "print(f\"  Training final accuracy: {training_metrics[-1]['accuracy']:.4f}\")\n",
        "print(\"\\nTest Results:\")\n",
        "print(f\"  Test Loss: {test_loss:.6f}\")\n",
        "print(f\"  Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "x62OR-ppGmCD",
        "outputId": "bec5935c-5516-4522-b54d-c9fa794c3390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes in __init__:\n",
            "W[0] shape: torch.Size([128, 1156])\n",
            "W[1] shape: torch.Size([64, 128])\n",
            "z[0] shape: torch.Size([300, 128, 128])\n",
            "z[1] shape: torch.Size([300, 128, 64])\n",
            "a[0] shape: torch.Size([300, 128, 128])\n",
            "a[1] shape: torch.Size([300, 128, 64])\n",
            "Sample data shape: torch.Size([128, 310, 2, 34, 34])\n",
            "Sample target shape: torch.Size([128])\n",
            "------ Warming Epoch: 0 ------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (128x64 and 128x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-a6859a70aea8>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarming_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Warming completed on batch {i+1}/{len(trainloader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Run warming on only one batch for simplicity; you could extend it if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-9bd86018f46a>\u001b[0m in \u001b[0;36mwarming\u001b[0;34m(self, inputs, labels, epochs, beta, gamma)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;31m# Update weights W_l using the weight update function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0;31m# Update activations (a) and membrane potentials (z) for each time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-9bd86018f46a>\u001b[0m in \u001b[0;36m_weight_update\u001b[0;34m(self, layer_output, activation_input)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mrho_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-9bd86018f46a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mrho_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x64 and 128x128)"
          ]
        }
      ]
    }
  ]
}