{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK1tqjGKPZgGdp1exViujh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ergysmedaunipd/thesis/blob/main/ThesisUnipdSNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tonic\n",
        "!pip install snntorch\n",
        "!pip install psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc3-4tqnv5AS",
        "outputId": "dd6400b9-c02b-4924-b507-90ada7590a86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tonic\n",
            "  Downloading tonic-1.5.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from tonic) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from tonic) (3.12.1)\n",
            "Collecting importRosbag>=1.0.4 (from tonic)\n",
            "  Downloading importRosbag-1.0.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tonic) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tonic) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from tonic) (4.12.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from tonic) (0.10.2.post1)\n",
            "Collecting pbr (from tonic)\n",
            "  Downloading pbr-6.1.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting expelliarmus (from tonic)\n",
            "  Downloading expelliarmus-1.1.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from importRosbag>=1.0.4->tonic) (75.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa->tonic) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->tonic) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->tonic) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->tonic) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->tonic) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->tonic) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tonic) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (2024.8.30)\n",
            "Downloading tonic-1.5.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importRosbag-1.0.4-py3-none-any.whl (28 kB)\n",
            "Downloading expelliarmus-1.1.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pbr, importRosbag, expelliarmus, tonic\n",
            "Successfully installed expelliarmus-1.1.12 importRosbag-1.0.4 pbr-6.1.0 tonic-1.5.0\n",
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.26.4)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-1.0.4-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.12.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (3.0.2)\n",
            "Downloading snntorch-0.9.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nir-1.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: nir, nirtorch, snntorch\n",
            "Successfully installed nir-1.0.4 nirtorch-1.0 snntorch-0.9.1\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "zcOCVUH7r4kE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import snntorch as snn\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class ADMM_SNN:\n",
        "\n",
        "    \"\"\" Class for ADMM Neural Network. \"\"\"\n",
        "\n",
        "    def __init__(self, n_samples: int, n_timesteps: int, input_dim: int, hidden_dims: List[int], n_outputs: int, rho: float, deltas: torch.Tensor, thetas: torch.Tensor):\n",
        "        self.device = \"cpu\"\n",
        "\n",
        "        # Define hyperparameters:\n",
        "        # - thetas = Thresholds (can be all the same or different for each neuron)\n",
        "        # - deltas = Decay factors (can be all the same or different for each neuron)\n",
        "        # - roh = Penalty parameter. All the \\alpha_{l,t} = \\beta_{l,t} = \\rho/2\n",
        "\n",
        "        self.rho = rho\n",
        "        self.deltas = deltas\n",
        "        self.thetas = thetas\n",
        "\n",
        "        self.L = len(hidden_dims)\n",
        "        self.T = n_timesteps\n",
        "\n",
        "        # Define a_0_t, which will be the input to the first layer\n",
        "        self.a0 = torch.zeros(\n",
        "            (n_timesteps, n_samples, input_dim)).to(self.device)\n",
        "\n",
        "        # === Initialize W_l ===\n",
        "        self.W = []\n",
        "\n",
        "        # Now define the weights for each layer\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            if i == 0:\n",
        "                self.W.append(torch.zeros(\n",
        "                    (hidden_dim, input_dim)).to(self.device))\n",
        "            else:\n",
        "                self.W.append(torch.zeros(\n",
        "                    (hidden_dim, hidden_dims[i-1])).to(self.device))\n",
        "\n",
        "        # === Initialize z_l ===\n",
        "        self.z = []\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            self.z.append(torch.zeros(\n",
        "                (n_timesteps, n_samples, hidden_dim)).to(self.device))\n",
        "\n",
        "        # === Initialize a_l ===\n",
        "        self.a = []\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            self.a.append(torch.zeros(\n",
        "                (n_timesteps, n_samples, hidden_dim)).to(self.device))\n",
        "\n",
        "        # Check how to make initialization?\n",
        "        self.lambda_lagrange = torch.zeros(\n",
        "            (n_samples, n_outputs)).to(self.device)\n",
        "\n",
        "        print(\"Shapes in __init__:\")\n",
        "        for i, W in enumerate(self.W):\n",
        "            print(f\"W[{i}] shape: {W.shape}\")\n",
        "        for i, z in enumerate(self.z):\n",
        "            print(f\"z[{i}] shape: {z.shape}\")\n",
        "        for i, a in enumerate(self.a):\n",
        "            print(f\"a[{i}] shape: {a.shape}\")\n",
        "\n",
        "    def _heaviside(self, x):\n",
        "        # Implement the Heaviside function that compares each element in x to self.thetas\n",
        "        # Returns 1 if the element in x exceeds the threshold in self.thetas, otherwise 0\n",
        "        return (x >= self.thetas).float()\n",
        "\n",
        "    # ============ W_{l} update functions ============\n",
        "    def _weight_update(self, layer_output, activation_input):\n",
        "        # Implements the weight update for layers 1 to L-1 as in line 2 of Algorithm 2 (Equation 4)\n",
        "        # where α_{l,t} = ρ/2\n",
        "\n",
        "        T = self.T\n",
        "        rho_half = self.rho / 2\n",
        "        numerator = sum(rho_half * layer_output[t] @ activation_input[t].T for t in range(T))\n",
        "        denominator = sum(rho_half * (activation_input[t] @ activation_input[t].T) for t in range(T))\n",
        "\n",
        "        return numerator @ torch.linalg.pinv(denominator)\n",
        "\n",
        "    def _weight_update_L(self, layer_output, activation_input):\n",
        "        # Implements the weight update for the output layer L as in line 10 of Algorithm 2 (Equation 6)\n",
        "        # where α_{L,t} = ρ/2 and includes the Lagrange multiplier λ\n",
        "\n",
        "        T = self.T\n",
        "        rho_half = self.rho / 2\n",
        "        lambda_term = -self.lambda_lagrange / self.rho\n",
        "\n",
        "        numerator = lambda_term @ activation_input[-1].T + sum(rho_half * layer_output[t] @ activation_input[t].T for t in range(T))\n",
        "        denominator = sum(rho_half * (activation_input[t] @ activation_input[t].T) for t in range(T))\n",
        "\n",
        "        return numerator @ torch.linalg.pinv(denominator)\n",
        "\n",
        "    # ============ z_{l,t} update functions ============\n",
        "    def _z_update(self, q, r, alpha_l, delta, a, t):\n",
        "        # Implements the z_{l,t} update for l = 1, ..., L - 1 and t < T (Equation 14)\n",
        "        # q and r are precomputed terms as defined in the equations\n",
        "\n",
        "        theta_a = self.thetas * a\n",
        "        delta_term = alpha_l[t + 1] * delta * (r + theta_a) if t < self.T - 1 else 0\n",
        "        numerator = alpha_l[t] * q + delta_term\n",
        "        denominator = alpha_l[t] + delta**2 * alpha_l[t + 1] if t < self.T - 1 else alpha_l[t]\n",
        "\n",
        "        return numerator / denominator\n",
        "\n",
        "    def _z_update_T(self, q, alpha_l, t):\n",
        "        # Implements the z_{l,T} update for l = 1, ..., L - 1 and t = T (Equation 14 with t = T)\n",
        "\n",
        "        return q  # The simplification for the last time step where t = T\n",
        "\n",
        "    def _z_update_L(self, s, r, alpha_l, delta, a, t):\n",
        "        # Implements the z_{L,t} update for l = L and t < T (Equation 16 with t < T)\n",
        "\n",
        "        theta_a = self.thetas * a\n",
        "        delta_term = alpha_l[t + 1] * delta * (r + theta_a) if t < self.T - 1 else 0\n",
        "        numerator = alpha_l[t] * s + delta_term\n",
        "        denominator = alpha_l[t] + delta**2 * alpha_l[t + 1] if t < self.T - 1 else alpha_l[t]\n",
        "\n",
        "        return numerator / denominator\n",
        "\n",
        "    def _z_update_L_T(self, s, y, alpha_l, rho, lambda_lagrange, t):\n",
        "        # Implements the z_{L,T} update for l = L and t = T (Equation 16 with t = T)\n",
        "\n",
        "        lambda_term = (y - lambda_lagrange / rho) if t == self.T - 1 else 0\n",
        "        numerator = alpha_l[t] * s + lambda_term\n",
        "        denominator = alpha_l[t] + 1 if t == self.T - 1 else alpha_l[t]\n",
        "\n",
        "        return numerator / denominator\n",
        "\n",
        "    def check_entries(self, z, cost_function):\n",
        "        # Implements Algorithm 1 from the document to check and modify entries in z if needed\n",
        "        for n in range(z.shape[0]):  # Assuming z has shape (N_l, M) where N_l is number of neurons\n",
        "            for m in range(z.shape[1]):  # M is the number of samples\n",
        "                if z[n, m] > self.thetas and cost_function(z[n, m]) > cost_function(self.thetas):\n",
        "                    z[n, m] = self.thetas\n",
        "                elif z[n, m] <= self.thetas and cost_function(z[n, m] + 1e-5) < cost_function(z[n, m]):\n",
        "                    z[n, m] = self.thetas + 1e-5\n",
        "        return z\n",
        "\n",
        "    # ============ a_{l,t} update functions ============\n",
        "    def _activation_update(self, wl, wl_next, alpha_l, beta_l, delta, theta, z, a_prev, t):\n",
        "        # Implement the Activation update for l=1,...,L-2, t=1,...,T-1 (line 4 of Algorithm 2)\n",
        "\n",
        "        rho_theta_I = (theta**2) * torch.eye(wl.size(0)).to(self.device)  # Term (θ^2) * I\n",
        "        WtW = wl_next.T @ wl_next\n",
        "        term1 = -theta * delta * (wl @ a_prev)  # Term with -θ * delta * wl\n",
        "        term2 = WtW + rho_theta_I + beta_l * torch.eye(wl.size(0)).to(self.device)  # Adding W_{l+1}^T W_{l+1} + (θ^2) * I\n",
        "        term3 = wl.T @ z[t + 1]  # Term with W_{l+1}^T * z_{l+1, t+1}\n",
        "\n",
        "        # Compute activation update\n",
        "        activation_update = torch.inverse(term2) @ (term1 + term3)\n",
        "\n",
        "        return activation_update\n",
        "\n",
        "    def _activation_update_T(self, wl, wl_next, alpha_l, beta_l, z, theta, a_prev, t):\n",
        "        # Implement the Activation update for l=1,...,L-2, t=T (line 7 of Algorithm 2)\n",
        "\n",
        "        term1 = wl.T @ (z[-1] - theta)  # Term W_{l+1}^T * z_{l+1, T}\n",
        "        WtW = wl_next.T @ wl_next\n",
        "        term2 = WtW + beta_l * torch.eye(wl.size(0)).to(self.device)  # Adding W_{l+1}^T W_{l+1}\n",
        "\n",
        "        # Compute activation update for last time step\n",
        "        activation_update_T = torch.inverse(term2) @ (term1 + wl.T)\n",
        "\n",
        "        return activation_update_T\n",
        "\n",
        "    def _activation_update_Lminus1(self, wl, wl_next, alpha_l, beta_l, delta, theta, z, a_prev, t):\n",
        "        # Implement the Activation update for l=L-1, t=1,...,T-1 (line 4 again, check Indicator functions)\n",
        "\n",
        "        rho_theta_I = (theta**2) * torch.eye(wl.size(0)).to(self.device)  # Term (θ^2) * I\n",
        "        WtW = wl_next.T @ wl_next\n",
        "        term1 = -theta * delta * (wl @ a_prev)  # Term with -θ * delta * wl\n",
        "        term2 = WtW + rho_theta_I + beta_l * torch.eye(wl.size(0)).to(self.device)  # Adding W_{l+1}^T W_{l+1} + (θ^2) * I\n",
        "        term3 = wl.T @ z[t + 1]  # Term with W_{l+1}^T * z_{l+1, t+1}\n",
        "\n",
        "        # Compute activation update for L-1 layer\n",
        "        activation_update_Lminus1 = torch.inverse(term2) @ (term1 + term3)\n",
        "\n",
        "        return activation_update_Lminus1\n",
        "\n",
        "    def _activation_update_Lminus1_T(self, wl, wl_next, alpha_l, beta_l, z, theta, a_prev, t):\n",
        "        # Implement the Activation update for l=L-1, t=T (line 7 again, check Indicator functions)\n",
        "\n",
        "        term1 = wl.T @ (z[-1] - theta)  # Term W_{l+1}^T * z_{l+1, T}\n",
        "        WtW = wl_next.T @ wl_next\n",
        "        term2 = WtW + beta_l * torch.eye(wl.size(0)).to(self.device)  # Adding W_{l+1}^T W_{l+1}\n",
        "\n",
        "        # Compute activation update for last layer at T\n",
        "        activation_update_Lminus1_T = torch.inverse(term2) @ (term1 + wl.T)\n",
        "\n",
        "        return activation_update_Lminus1_T\n",
        "\n",
        "    # ============ lagrange multiplier update ============\n",
        "    def _lambda_update(self, zL_T, delta_zL_T_minus_1, WL, aL_minus_1_T, rho):\n",
        "        # Implement the update of the Lagrange multiplier lambda (Line 15 of Algorithm 2)\n",
        "\n",
        "        # Calculate the term inside the parentheses: zL,T - delta * zL,T-1 - WL * aL-1,T\n",
        "        term = zL_T - delta_zL_T_minus_1 - WL @ aL_minus_1_T\n",
        "\n",
        "        # Update lambda using the formula in Line 15\n",
        "        lambda_update = self.lambda_lagrange + rho * term\n",
        "\n",
        "        return lambda_update\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        # Ensure inputs are on the correct device\n",
        "        inputs = inputs.to(self.device)\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        mem_states = [snn.Leaky(beta=self.deltas[i]).init_leaky() for i in range(self.L)]\n",
        "\n",
        "        # Record outputs for the final layer\n",
        "        mem_rec = []\n",
        "\n",
        "        # Run the simulation for each timestep\n",
        "        for t in range(self.T):\n",
        "            current_input = inputs[t] if t < inputs.size(0) else torch.zeros_like(inputs[0])\n",
        "\n",
        "            # Process through each layer\n",
        "            for l in range(self.L):\n",
        "                # Calculate post-synaptic current for the current layer\n",
        "                if l == 0:\n",
        "                    post_syn_current = current_input @ self.W[l].T\n",
        "                else:\n",
        "                    post_syn_current = self.a[l - 1][t] @ self.W[l].T\n",
        "\n",
        "                # Apply snn.Leaky (LIF neuron model) to the post-synaptic current\n",
        "                lif_neuron = snn.Leaky(beta=self.deltas[l], threshold=self.thetas[l],\n",
        "                                       reset_mechanism='subtract' if l < self.L - 1 else 'none').to(self.device)\n",
        "                mem, mem_states[l] = lif_neuron(post_syn_current, mem_states[l])\n",
        "\n",
        "                # Apply Heaviside function to determine if there is a spike\n",
        "                spike = self._heaviside(mem)\n",
        "\n",
        "                # Store membrane potential and spike\n",
        "                self.z[l][t] = mem  # Membrane potential\n",
        "                self.a[l][t] = spike  # Binary spike output\n",
        "\n",
        "            # Record membrane potential of the last layer for the final timestep\n",
        "            if t == self.T - 1:\n",
        "                mem_rec.append(self.z[-1][t])\n",
        "\n",
        "        # Return the membrane potentials of the last layer at the final timestep\n",
        "        return mem_rec[-1]\n",
        "\n",
        "    def fit(self, inputs):\n",
        "        # This function updates the optimization variables, given an input batch of data samples.\n",
        "\n",
        "        # Carry out the updates following algorithm (2)\n",
        "\n",
        "        # Here is a skeleton of the implementation:\n",
        "        for l in range(1, self.L):\n",
        "            # Update self.W[l] using the function _weight_update\n",
        "            self.W[l] = self._weight_update(self.z[l], self.a[l - 1])\n",
        "\n",
        "            for t in range(1, self.T):\n",
        "                if l < self.L - 1:\n",
        "                    self.a[l][t] = self._activation_update(self.W[l], self.W[l + 1], self.rho / 2, self.rho / 2, self.deltas[l], self.thetas[l], self.z[l], self.a[l - 1][t], t)\n",
        "                else:\n",
        "                    self.a[l][t] = self._activation_update_Lminus1(self.W[l], self.W[l + 1], self.rho / 2, self.rho / 2, self.deltas[l], self.thetas[l], self.z[l], self.a[l - 1][t], t)\n",
        "\n",
        "                # update selfz[l][t] using the function _z_update and check_entries\n",
        "                self.z[l][t] = self.check_entries(self._z_update(self.z[l][t], self.a[l][t], self.rho / 2, self.deltas[l], self.a[l][t], t), self._heaviside)\n",
        "\n",
        "            if l < self.L-1:\n",
        "                self.a[l][self.T] = self._activation_update_T(self.W[l], self.W[l + 1], self.rho / 2, self.rho / 2, self.z[l], self.thetas[l], self.a[l - 1][self.T - 1], self.T)\n",
        "            else:\n",
        "                # update self.a[l][T] using the function _activation_update_Lminus1_T\n",
        "                self.a[l][self.T] = self._activation_update_Lminus1_T(self.W[l], self.W[l + 1], self.rho / 2, self.rho / 2, self.z[l], self.thetas[l], self.a[l - 1][self.T - 1], self.T)\n",
        "\n",
        "\n",
        "            # update self.z[l][T] using the function _z_update_T and check_entries\n",
        "            self.z[l][self.T] = self.check_entries(self._z_update_T(self.z[l][self.T], self.rho / 2, self.T), self._heaviside)\n",
        "\n",
        "\n",
        "        # ----- Update the last layer -----\n",
        "        # Update self.W[L] using the function _weight_update_L\n",
        "        self.W[self.L] = self._weight_update_L(self.z[self.L], self.a[self.L - 1])\n",
        "        for t in range(1, self.T):\n",
        "            # update self.z[L][t] using the function _z_update_L\n",
        "            self.z[self.L][t] = self._z_update_L(self.z[self.L][t], self.a[self.L][t], self.rho / 2, self.deltas[self.L], self.a[self.L][t], t)\n",
        "\n",
        "        # ----- Update the last layer at time T -----\n",
        "        # update self.z[L][T] using the function _z_update_L_T\n",
        "        self.z[self.L][self.T] = self._z_update_L_T(self.z[self.L][self.T], inputs, self.rho, self.lambda_lagrange, self.T)\n",
        "\n",
        "        # Update the lagrange multiplier using the function _lambda_update\n",
        "        self.lambda_lagrange = self._lambda_update(self.z[self.L][self.T], self.deltas[self.L] * self.z[self.L][self.T - 1], self.W[self.L], self.a[self.L - 1][self.T - 1], self.rho)\n",
        "        return\n",
        "\n",
        "    def evaluate(self, inputs):\n",
        "        \"\"\"\n",
        "        Standard evaluation phase.\n",
        "\n",
        "        Args:\n",
        "            inputs (torch.Tensor): Input data batch to be evaluated.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output membrane potentials or activations of the last layer.\n",
        "        \"\"\"\n",
        "        # Ensure inputs are on the correct device\n",
        "        inputs = inputs.to(self.device)\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        mem_states = [snn.Leaky(beta=self.deltas[i]).init_leaky() for i in range(self.L)]\n",
        "\n",
        "        # Record the output of the final layer for each time step\n",
        "        mem_rec = []\n",
        "\n",
        "        # Run the simulation for each timestep\n",
        "        for t in range(self.T):\n",
        "            current_input = inputs[t] if t < inputs.size(0) else torch.zeros_like(inputs[0])\n",
        "\n",
        "            # Process through each layer\n",
        "            for l in range(self.L):\n",
        "                # Calculate post-synaptic current for the current layer\n",
        "                if l == 0:\n",
        "                    post_syn_current = current_input @ self.W[l].T\n",
        "                else:\n",
        "                    post_syn_current = self.a[l - 1][t] @ self.W[l].T\n",
        "\n",
        "                # Apply snn.Leaky (LIF neuron model) to the post-synaptic current\n",
        "                lif_neuron = snn.Leaky(beta=self.deltas[l], threshold=self.thetas[l],\n",
        "                                       reset_mechanism='subtract' if l < self.L - 1 else 'none').to(self.device)\n",
        "                mem, mem_states[l] = lif_neuron(post_syn_current, mem_states[l])\n",
        "\n",
        "                # Apply Heaviside function to determine if there is a spike\n",
        "                spike = self._heaviside(mem)\n",
        "\n",
        "                # Store membrane potential and spike\n",
        "                self.z[l][t] = mem  # Membrane potential\n",
        "                self.a[l][t] = spike  # Binary spike output\n",
        "\n",
        "            # Record the membrane potential of the last layer at each time step\n",
        "            if l == self.L - 1:\n",
        "                mem_rec.append(self.z[l][t])\n",
        "\n",
        "        # Return the final membrane potentials (or spikes) of the last layer at each time step\n",
        "        return torch.stack(mem_rec, dim=0)\n",
        "\n",
        "    def warming(self, inputs, labels, epochs, beta, gamma):\n",
        "        \"\"\"\n",
        "        Warming phase for ADMM SNN by minimizing sub-problems without updating lambda.\n",
        "\n",
        "        Args:\n",
        "            inputs (torch.Tensor): Input data samples for training.\n",
        "            labels (torch.Tensor): Labels for training data samples.\n",
        "            epochs (int): Number of warming-up epochs.\n",
        "            beta (float): Value of beta for activation update.\n",
        "            gamma (float): Value of gamma for membrane potential update.\n",
        "\n",
        "        \"\"\"\n",
        "        self.a0 = inputs  # Set initial input layer activations\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"------ Warming Epoch: {epoch} ------\")\n",
        "\n",
        "            # Iterate over each layer\n",
        "            for l in range(1, self.L):\n",
        "                # Update weights W_l using the weight update function\n",
        "                self.W[l] = self._weight_update(self.z[l], self.a[l - 1])\n",
        "\n",
        "                # Update activations (a) and membrane potentials (z) for each time step\n",
        "                for t in range(1, self.T):\n",
        "                    if l < self.L - 1:\n",
        "                        # Intermediate layer activation update\n",
        "                        self.a[l][t] = self._activation_update(self.W[l], self.W[l + 1], beta, gamma, self.deltas[l], self.thetas[l], self.z[l], self.a[l - 1][t], t)\n",
        "                    else:\n",
        "                        # Last hidden layer activation update\n",
        "                        self.a[l][t] = self._activation_update_Lminus1(self.W[l], self.W[l + 1], beta, gamma, self.deltas[l], self.thetas[l], self.z[l], self.a[l - 1][t], t)\n",
        "\n",
        "                    # Update z for current layer and time step\n",
        "                    self.z[l][t] = self._z_update(self.z[l][t], self.a[l][t], beta, self.deltas[l], self.a[l][t], t)\n",
        "\n",
        "                # Update activations and membrane potential for the last time step T\n",
        "                if l < self.L - 1:\n",
        "                    self.a[l][self.T] = self._activation_update_T(self.W[l], self.W[l + 1], beta, gamma, self.z[l], self.thetas[l], self.a[l - 1][self.T - 1], self.T)\n",
        "                else:\n",
        "                    self.a[l][self.T] = self._activation_update_Lminus1_T(self.W[l], self.W[l + 1], beta, gamma, self.z[l], self.thetas[l], self.a[l - 1][self.T - 1], self.T)\n",
        "\n",
        "                # Update z for the last time step T\n",
        "                self.z[l][self.T] = self._z_update_T(self.z[l][self.T], beta, self.T)\n",
        "\n",
        "            # ----- Warming up the last layer -----\n",
        "            # Update weights for the output layer\n",
        "            self.W[self.L] = self._weight_update_L(self.z[self.L], self.a[self.L - 1])\n",
        "\n",
        "            for t in range(1, self.T):\n",
        "                # Update z in the last layer for each time step\n",
        "                self.z[self.L][t] = self._z_update_L(self.z[self.L][t], self.a[self.L][t], beta, self.deltas[self.L], self.a[self.L][t], t)\n",
        "\n",
        "            # Update z for the last layer at time T\n",
        "            self.z[self.L][self.T] = self._z_update_L_T(self.z[self.L][self.T], labels, beta, self.lambda_lagrange, self.T)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import tonic\n",
        "import tonic.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tonic import DiskCachedDataset\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations\n",
        "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
        "frame_transform = transforms.Compose([\n",
        "    transforms.Denoise(filter_time=10000),\n",
        "    transforms.ToFrame(sensor_size=sensor_size, time_window=1000)\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
        "testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)\n",
        "\n",
        "# Cache datasets\n",
        "cached_trainset = DiskCachedDataset(trainset, cache_path='./cache/nmnist/train')\n",
        "cached_testset = DiskCachedDataset(testset, cache_path='./cache/nmnist/test')\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 128\n",
        "trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(), shuffle=True)\n",
        "testloader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYuF2XdhGXZg",
        "outputId": "5a36d8f8-4d58-4110-ed23-99bdc23b7077"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes in __init__:\n",
            "W[0] shape: torch.Size([100, 1156])\n",
            "W[1] shape: torch.Size([50, 100])\n",
            "z[0] shape: torch.Size([300, 128, 100])\n",
            "z[1] shape: torch.Size([300, 128, 50])\n",
            "a[0] shape: torch.Size([300, 128, 100])\n",
            "a[1] shape: torch.Size([300, 128, 50])\n",
            "Sample data shape: torch.Size([128, 309, 2, 34, 34])\n",
            "Sample target shape: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkSmtX6THwAq",
        "outputId": "be39b244-e3ed-4db2-d4fb-bf6b18c8d648"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes in __init__:\n",
            "W[0] shape: torch.Size([100, 1156])\n",
            "W[1] shape: torch.Size([50, 100])\n",
            "z[0] shape: torch.Size([300, 128, 100])\n",
            "z[1] shape: torch.Size([300, 128, 50])\n",
            "a[0] shape: torch.Size([300, 128, 100])\n",
            "a[1] shape: torch.Size([300, 128, 50])\n",
            "Sample data shape: torch.Size([128, 311, 2, 34, 34])\n",
            "Sample target shape: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the ADMM SNN model\n",
        "n_samples = batch_size\n",
        "n_timesteps = 300  # Adjust based on data\n",
        "input_dim = sensor_size[0] * sensor_size[1]\n",
        "hidden_dims = [128, 64]  # Example hidden layer dimensions\n",
        "n_outputs = 10\n",
        "rho = 0.5\n",
        "deltas = torch.tensor([0.9, 0.8, 0.7])  # Adjust for each layer\n",
        "thetas = torch.tensor([1.0, 1.0, 1.0])  # Adjust for each layer\n",
        "\n",
        "model = ADMM_SNN(n_samples, n_timesteps, input_dim, hidden_dims, n_outputs, rho, deltas, thetas)\n",
        "\n",
        "\n",
        "sample_data, sample_target = next(iter(trainloader))\n",
        "print(\"Sample data shape:\", sample_data.shape)\n",
        "print(\"Sample target shape:\", sample_target.shape)\n",
        "\n",
        "# Warming phase parameters\n",
        "warming_epochs = 5\n",
        "beta = 0.5\n",
        "gamma = 0.5\n",
        "\n",
        "# Perform the warming phase\n",
        "for i, (data, targets) in enumerate(trainloader):\n",
        "    data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "    # Flatten spatial dimensions\n",
        "    data = data.view(data.size(0), data.size(1), -1)\n",
        "    if data.size(1) > 300:\n",
        "        data = data[:, :300, :]\n",
        "\n",
        "    model.warming(data, targets, epochs=warming_epochs, beta=beta, gamma=gamma)\n",
        "    print(f\"Warming completed on batch {i+1}/{len(trainloader)}\")\n",
        "    break  # Run warming on only one batch for simplicity; you could extend it if needed\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "x62OR-ppGmCD",
        "outputId": "bec5935c-5516-4522-b54d-c9fa794c3390"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes in __init__:\n",
            "W[0] shape: torch.Size([128, 1156])\n",
            "W[1] shape: torch.Size([64, 128])\n",
            "z[0] shape: torch.Size([300, 128, 128])\n",
            "z[1] shape: torch.Size([300, 128, 64])\n",
            "a[0] shape: torch.Size([300, 128, 128])\n",
            "a[1] shape: torch.Size([300, 128, 64])\n",
            "Sample data shape: torch.Size([128, 310, 2, 34, 34])\n",
            "Sample target shape: torch.Size([128])\n",
            "------ Warming Epoch: 0 ------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (128x64 and 128x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-a6859a70aea8>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarming_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Warming completed on batch {i+1}/{len(trainloader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Run warming on only one batch for simplicity; you could extend it if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-9bd86018f46a>\u001b[0m in \u001b[0;36mwarming\u001b[0;34m(self, inputs, labels, epochs, beta, gamma)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;31m# Update weights W_l using the weight update function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0;31m# Update activations (a) and membrane potentials (z) for each time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-9bd86018f46a>\u001b[0m in \u001b[0;36m_weight_update\u001b[0;34m(self, layer_output, activation_input)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mrho_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-9bd86018f46a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mrho_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x64 and 128x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "num_epochs = 1\n",
        "\n",
        "# Main ADMM training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data, targets) in enumerate(trainloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Flatten spatial dimensions\n",
        "        data = data.view(data.size(0), data.size(1), -1)\n",
        "        if data.size(1) > 300:\n",
        "            data = data[:, :300, :]\n",
        "\n",
        "        # Perform ADMM optimization using the `fit` method\n",
        "        model.fit(data)\n",
        "\n",
        "        # Compute loss (for monitoring purposes only)\n",
        "        spk_rec = model.evaluate(data)\n",
        "        loss_val = SF.mse_count_loss(spk_rec, targets)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], Loss: {loss_val.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "total = 0\n",
        "correct = 0\n",
        "for data, targets in testloader:\n",
        "    data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "    # Flatten spatial dimensions\n",
        "    data = data.view(data.size(0), data.size(1), -1)\n",
        "    if data.size(1) > 300:\n",
        "        data = data[:, :300, :]\n",
        "\n",
        "    # Forward pass using `evaluate` method of ADMM_SNN\n",
        "    spk_rec = model.evaluate(data)\n",
        "    _, predicted = spk_rec.sum(dim=0).max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += (predicted == targets).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "9U-3LFWeITpF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}