{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHL0JVDGEdBSOgeQtCsGXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ergysmedaunipd/thesis/blob/main/ThesisUnipdSNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tonic\n",
        "!pip install snntorch\n",
        "!pip install psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc3-4tqnv5AS",
        "outputId": "dd6400b9-c02b-4924-b507-90ada7590a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tonic\n",
            "  Downloading tonic-1.5.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from tonic) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from tonic) (3.12.1)\n",
            "Collecting importRosbag>=1.0.4 (from tonic)\n",
            "  Downloading importRosbag-1.0.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tonic) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tonic) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from tonic) (4.12.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from tonic) (0.10.2.post1)\n",
            "Collecting pbr (from tonic)\n",
            "  Downloading pbr-6.1.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting expelliarmus (from tonic)\n",
            "  Downloading expelliarmus-1.1.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from importRosbag>=1.0.4->tonic) (75.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa->tonic) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->tonic) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->tonic) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->tonic) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->tonic) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->tonic) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tonic) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (2024.8.30)\n",
            "Downloading tonic-1.5.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importRosbag-1.0.4-py3-none-any.whl (28 kB)\n",
            "Downloading expelliarmus-1.1.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pbr, importRosbag, expelliarmus, tonic\n",
            "Successfully installed expelliarmus-1.1.12 importRosbag-1.0.4 pbr-6.1.0 tonic-1.5.0\n",
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.26.4)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-1.0.4-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.12.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (3.0.2)\n",
            "Downloading snntorch-0.9.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nir-1.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: nir, nirtorch, snntorch\n",
            "Successfully installed nir-1.0.4 nirtorch-1.0 snntorch-0.9.1\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcOCVUH7r4kE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class ADMM_SNN:\n",
        "    \"\"\" Class for ADMM Neural Network. \"\"\"\n",
        "\n",
        "    def __init__(self, n_samples: int, n_timesteps: int, input_dim: int, hidden_dims: List[int], n_outputs: int, rho: float, delta: float, theta: float):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        # Define hyperparameters:\n",
        "        self.rho = rho\n",
        "        self.delta = delta\n",
        "        self.theta = theta\n",
        "\n",
        "        self.L = len(hidden_dims)  # Number of layers\n",
        "        self.T = n_timesteps       # Number of timesteps\n",
        "\n",
        "        self.a0 = torch.zeros((n_timesteps, n_samples, input_dim)).to(self.device)\n",
        "        print(f\"Initialized a0 with shape: {self.a0.shape}\")  # Debugging\n",
        "\n",
        "        # Initialize W_l (weights for each layer)\n",
        "        self.W = nn.ParameterList()\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "                if i == 0:\n",
        "                    self.W.append(nn.Parameter(\n",
        "                        torch.randn((hidden_dim, input_dim)).to(self.device) *\n",
        "                        np.sqrt(2.0 / input_dim)\n",
        "                    ))\n",
        "                else:\n",
        "                    self.W.append(nn.Parameter(\n",
        "                        torch.randn((hidden_dim, hidden_dims[i-1])).to(self.device) *\n",
        "                        np.sqrt(2.0 / hidden_dims[i-1])\n",
        "                    ))\n",
        "\n",
        "        # Initialize z_l (intermediate variables for each layer and timestep)\n",
        "        self.z = [torch.rand((n_timesteps, n_samples, hidden_dim)).to(self.device) for hidden_dim in hidden_dims]\n",
        "        for i, z_layer in enumerate(self.z):\n",
        "            print(f\"Initialized z[{i}] with shape: {z_layer.shape}\")  # Debugging\n",
        "\n",
        "        # Initialize a_l (activations for each layer and timestep)\n",
        "        self.a = [torch.rand((n_timesteps, n_samples, hidden_dim)).to(self.device) for hidden_dim in hidden_dims]\n",
        "        for i, a_layer in enumerate(self.a):\n",
        "            print(f\"Initialized a[{i}] with shape: {a_layer.shape}\")  # Debugging\n",
        "\n",
        "        # Initialize lambda (Lagrange multipliers for the output layer)\n",
        "        self.lambda_lagrange = torch.ones((n_samples, n_outputs)).to(self.device)\n",
        "        print(f\"Initialized lambda_lagrange with shape: {self.lambda_lagrange.shape}\")  # Debugging\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        model_str = \"ADMM SNN Model Structure:\\n\"\n",
        "        model_str += f\" - rho: {self.rho}, delta: {self.delta}, theta: {self.theta}\\n\"\n",
        "        model_str += f\" - Number of timesteps: {self.T}\\n\"\n",
        "        model_str += f\" - Input dimension: {self.a0.size()}\\n\"\n",
        "        model_str += f\" - Hidden layers: {[w.shape for w in self.W]}\\n\"\n",
        "        model_str += f\" - Output dimension (Lagrange Multiplier): {self.lambda_lagrange.size()}\\n\"\n",
        "        return model_str\n",
        "\n",
        "    def _heaviside(self, z):\n",
        "        \"\"\"\n",
        "        Implement the Heaviside function to calculate activations based on a single threshold.\n",
        "        This function returns 1 where z exceeds the threshold and 0 otherwise.\n",
        "\n",
        "        Parameters:\n",
        "        z (torch.Tensor): The input tensor (intermediate variable z at layer l and time t).\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: A tensor of the same shape as z with values 0 or 1 based on the threshold self.theta.\n",
        "        \"\"\"\n",
        "        return (z >= self.theta).float()\n",
        "\n",
        "    # ============ W_{l} update functions ============\n",
        "    def _weight_update(self, z_l, a_l_minus_1):\n",
        "        \"\"\"\n",
        "        Update the weights for intermediate layers (1 to L-1) based on Equation (4).\n",
        "        Parameters:\n",
        "        - z_l (torch.Tensor): Shape [T, batch, n_features]\n",
        "        - a_l_minus_1 (torch.Tensor): Shape [T, batch, n_prev_features]\n",
        "\n",
        "        Returns:\n",
        "        - Updated weights for the layer.\n",
        "        \"\"\"\n",
        "        # Define alpha as rho / 2\n",
        "\n",
        "        alpha = self.rho / 2\n",
        "\n",
        "        # Get dimensions\n",
        "        n_timesteps, batch_size, n_features = z_l.shape\n",
        "        _, _, n_prev_features = a_l_minus_1.shape\n",
        "\n",
        "        # Initialize accumulator tensors\n",
        "        numerator = torch.zeros((n_features, n_prev_features), device=self.device)\n",
        "        denominator = torch.zeros((n_prev_features, n_prev_features), device=self.device)\n",
        "\n",
        "        # Iterate over timesteps\n",
        "        for t in range(n_timesteps):\n",
        "            if t > 0:\n",
        "                # For each timestep after 0, compute the adjusted z term\n",
        "                # All terms should maintain their original dimensions\n",
        "                current_z = z_l[t]  # [batch, n_features]\n",
        "                prev_z = z_l[t - 1]  # [batch, n_features]\n",
        "                current_a = a_l_minus_1[t]  # [batch, n_prev_features]\n",
        "                prev_a = a_l_minus_1[t - 1]  # [batch, n_prev_features]\n",
        "\n",
        "                # Compute z term adjustments within the same feature space\n",
        "                z_diff = current_z - self.delta * prev_z  # [batch, n_features]\n",
        "\n",
        "                # Add contribution to numerator\n",
        "                # Use einsum to handle the batch dimension properly\n",
        "                term = torch.einsum('bf,bp->fp', z_diff, current_a)\n",
        "                numerator += alpha * term\n",
        "\n",
        "                # Add theta * prev_a term separately to avoid dimension mismatch\n",
        "                theta_term = torch.einsum('bp,bf->fp', prev_a, z_diff)\n",
        "                numerator += alpha * self.theta * theta_term\n",
        "            else:\n",
        "                # For t=0, use simple matrix multiplication\n",
        "                term = torch.einsum('bf,bp->fp', z_l[0], a_l_minus_1[0])\n",
        "                numerator += alpha * term\n",
        "\n",
        "            # Update denominator using current timestep activations\n",
        "            denom_term = torch.einsum('bp,bq->pq', a_l_minus_1[t], a_l_minus_1[t])\n",
        "            denominator += alpha * denom_term\n",
        "\n",
        "        # Add small value to diagonal of denominator for numerical stability\n",
        "        denominator += torch.eye(denominator.shape[0], device=self.device) * 1e-6\n",
        "\n",
        "        # Compute final weight update\n",
        "        updated_weights = numerator @ torch.inverse(denominator)\n",
        "\n",
        "        return updated_weights\n",
        "\n",
        "    def _weight_update_L(self, z_L, a_L_minus_1, y):\n",
        "        \"\"\"\n",
        "        Update the weights for the final layer (L) based on Equation (6).\n",
        "        \"\"\"\n",
        "        # Define alpha as rho / 2\n",
        "        alpha = self.rho / 2\n",
        "\n",
        "        # Get dimensions\n",
        "        n_timesteps, batch_size, n_features = z_L.shape\n",
        "        _, _, n_prev_features = a_L_minus_1.shape\n",
        "\n",
        "        # Initialize accumulator tensors\n",
        "        numerator = torch.zeros((n_features, n_prev_features), device=self.device)\n",
        "        denominator = torch.zeros((n_prev_features, n_prev_features), device=self.device)\n",
        "\n",
        "        # Handle the last timestep separately for Lagrange multiplier term\n",
        "        a_L_minus_1_T = a_L_minus_1[-1]  # [batch, n_prev_features]\n",
        "\n",
        "        # Project lambda and y to match feature dimensions if needed\n",
        "        if self.lambda_lagrange.shape[1] != n_features:\n",
        "            # Project lambda from [batch, 10] to [batch, n_features]\n",
        "            lambda_proj = torch.zeros((batch_size, n_features), device=self.device)\n",
        "            lambda_proj[:, :self.lambda_lagrange.shape[1]] = self.lambda_lagrange\n",
        "\n",
        "            # Project y from [10, batch] to [batch, n_features]\n",
        "            y = y.t()  # [batch, 10]\n",
        "            y_proj = torch.zeros((batch_size, n_features), device=self.device)\n",
        "            y_proj[:, :y.shape[1]] = y\n",
        "        else:\n",
        "            lambda_proj = self.lambda_lagrange\n",
        "            y_proj = y.t()\n",
        "\n",
        "        # Compute Lagrange multiplier contribution with correct dimensions\n",
        "        lambda_term = torch.einsum('bf,bp->fp', lambda_proj, a_L_minus_1_T) / 2\n",
        "        numerator += lambda_term\n",
        "\n",
        "        # Process all timesteps\n",
        "        for t in range(n_timesteps):\n",
        "            if t > 0:\n",
        "                # Compute z term adjustments\n",
        "                z_diff = z_L[t] - self.delta * z_L[t - 1]  # [batch, n_features]\n",
        "\n",
        "                # Update numerator\n",
        "                term = torch.einsum('bf,bp->fp', z_diff, a_L_minus_1[t])\n",
        "                numerator += alpha * term\n",
        "            else:\n",
        "                # First timestep\n",
        "                term = torch.einsum('bf,bp->fp', z_L[0], a_L_minus_1[0])\n",
        "                numerator += alpha * term\n",
        "\n",
        "            # Update denominator\n",
        "            denom_term = torch.einsum('bp,bq->pq', a_L_minus_1[t], a_L_minus_1[t])\n",
        "            denominator += alpha * denom_term\n",
        "\n",
        "        # Add small value to diagonal of denominator for numerical stability\n",
        "        denominator += torch.eye(denominator.shape[0], device=self.device) * 1e-6\n",
        "\n",
        "        # Compute final weight update\n",
        "        updated_weights = numerator @ torch.inverse(denominator)\n",
        "\n",
        "        return updated_weights\n",
        "    # ============ z_{l,t} update functions ============\n",
        "    def _z_update(self, l, t):\n",
        "        \"\"\"\n",
        "        Update z_{l,t} for l = 1, ..., L-1 and t = 1, ..., T-1 based on Equation (14).\n",
        "        Now handles batched operations properly.\n",
        "        \"\"\"\n",
        "        alpha = self.rho / 2\n",
        "        q_l_t = self._calculate_q_l(l, t)  # Shape: [batch_size, n_features]\n",
        "        r_l_t_plus1 = self._calculate_r_l(l, t + 1)  # Shape: [batch_size, n_features]\n",
        "\n",
        "        # Compute components for update\n",
        "        if t < self.T:\n",
        "            a_term = self.theta * self.a[l][t]  # Shape: [batch_size, n_features]\n",
        "            r_term = r_l_t_plus1 + a_term\n",
        "            numerator = alpha * q_l_t + alpha * self.delta * r_term\n",
        "            denominator = alpha + self.delta**2 * alpha\n",
        "        else:\n",
        "            numerator = alpha * q_l_t\n",
        "            denominator = alpha\n",
        "\n",
        "        # Perform division (broadcasting handles batch dimension)\n",
        "        z_update = numerator / denominator\n",
        "\n",
        "        return z_update\n",
        "\n",
        "    def _z_update_T(self, l):\n",
        "        \"\"\"\n",
        "        Update z_{l,T} for l = 1, ..., L-1 based on Equation (14)*.\n",
        "        Applies Heaviside function for thresholding.\n",
        "        \"\"\"\n",
        "        alpha = self.rho / 2\n",
        "        q_l_T = self._calculate_q_l(l, self.T)  # Get q_{l,T}\n",
        "        r_l_T_minus1 = self._calculate_r_l(l, self.T - 1)  # Get r_{l,T-1}\n",
        "\n",
        "        # Compute the update for z_{l,T} based on Equation (14)* with the Heaviside function\n",
        "        numerator = alpha * q_l_T + alpha * self.delta * (r_l_T_minus1 + self.theta * self.a[l][self.T - 1])\n",
        "        denominator = alpha + self.delta**2 * alpha\n",
        "        z_update_T = numerator / denominator\n",
        "\n",
        "        # Apply Heaviside function to enforce thresholding\n",
        "        return self._heaviside(z_update_T)\n",
        "\n",
        "    def _z_update_L(self, y):\n",
        "        \"\"\"\n",
        "        Update z_{L,T} for the last layer L based on Equation (16).\n",
        "        Following similar structure to _z_update with proper batch handling.\n",
        "        \"\"\"\n",
        "        alpha = self.rho / 2\n",
        "        s_L_T = self._calculate_s_L()  # Shape: [batch_size, n_features]\n",
        "        r_L_T_minus1 = self._calculate_r_l(self.L - 1, self.T - 1)  # Shape: [batch_size, n_features]\n",
        "\n",
        "        # Transpose y to match batch dimension\n",
        "        y = y.t()  # [128, 10]\n",
        "\n",
        "        # Compute terms with proper broadcasting\n",
        "        numerator = alpha * s_L_T  # [128, 64]\n",
        "        r_term = r_L_T_minus1  # [128, 64]\n",
        "\n",
        "        # Project y and lambda to feature space if needed\n",
        "        if y.shape[1] != r_term.shape[1]:\n",
        "            W_L = self.W[self.L-1]  # [64, 128]\n",
        "            y = torch.matmul(y, W_L[:, :y.shape[1]].t())  # [128, 64]\n",
        "            lambda_term = torch.matmul(self.lambda_lagrange, W_L[:, :self.lambda_lagrange.shape[1]].t())  # [128, 64]\n",
        "        else:\n",
        "            lambda_term = self.lambda_lagrange\n",
        "\n",
        "        # Add terms with proper broadcasting\n",
        "        r_term = r_term + y - lambda_term / 2  # [128, 64]\n",
        "        numerator = numerator + alpha * self.delta * r_term  # [128, 64]\n",
        "        denominator = alpha + self.delta**2 * alpha  # scalar\n",
        "\n",
        "        # Compute final update\n",
        "        z_update = numerator / denominator  # [128, 64]\n",
        "\n",
        "        # Apply Heaviside function to enforce thresholding\n",
        "        return self._heaviside(z_update)\n",
        "\n",
        "    def _z_update_L_T(self, y):\n",
        "        \"\"\"\n",
        "        Update z_{L,T} for the last layer L at time T.\n",
        "        Following same structure as _z_update_L.\n",
        "        \"\"\"\n",
        "        alpha = self.rho / 2\n",
        "        s_L_T = self._calculate_s_L()  # [batch_size, n_features]\n",
        "        r_L_T = self._calculate_r_l(self.L - 1, self.T - 1)  # [batch_size, n_features]\n",
        "\n",
        "        # Transpose y to match batch dimension\n",
        "        y = y.t()  # [128, 10]\n",
        "\n",
        "        # Compute terms with proper broadcasting\n",
        "        numerator = alpha * s_L_T  # [128, 64]\n",
        "        r_term = r_L_T  # [128, 64]\n",
        "\n",
        "        # Project y and lambda to feature space if needed\n",
        "        if y.shape[1] != r_term.shape[1]:\n",
        "            W_L = self.W[self.L-1]  # [64, 128]\n",
        "            y = torch.matmul(y, W_L[:, :y.shape[1]].t())  # [128, 64]\n",
        "            lambda_term = torch.matmul(self.lambda_lagrange, W_L[:, :self.lambda_lagrange.shape[1]].t())  # [128, 64]\n",
        "        else:\n",
        "            lambda_term = self.lambda_lagrange\n",
        "\n",
        "        # Add terms with proper broadcasting\n",
        "        r_term = r_term + y - lambda_term / 2  # [128, 64]\n",
        "        numerator = numerator + alpha * self.delta * r_term  # [128, 64]\n",
        "        denominator = alpha + self.delta**2 * alpha  # scalar\n",
        "\n",
        "        # Compute final update\n",
        "        z_update = numerator / denominator  # [128, 64]\n",
        "\n",
        "\n",
        "        # Apply Heaviside function to enforce thresholding\n",
        "        return self._heaviside(z_update)\n",
        "\n",
        "    def check_entries(self, z, cost_function):\n",
        "        \"\"\"\n",
        "        Implements Algorithm 1 to check and adjust entries in z_{l,t}.\n",
        "        Handles batched operations properly.\n",
        "\n",
        "        Parameters:\n",
        "        - z (torch.Tensor): Input tensor with shape [batch_size, n_features]\n",
        "        - cost_function: Function to compute cost (e.g., torch.norm)\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Adjusted tensor with same shape as input\n",
        "        \"\"\"\n",
        "        # Get original shape and reshape if needed\n",
        "        original_shape = z.shape\n",
        "        if len(original_shape) > 2:\n",
        "            z = z.view(-1, original_shape[-1])\n",
        "\n",
        "        # Create a copy to modify\n",
        "        z_adjusted = z.clone()\n",
        "\n",
        "        # Get mask for values above threshold\n",
        "        above_threshold = z > self.theta\n",
        "\n",
        "        # Compute costs for current values and threshold\n",
        "        current_costs = cost_function(z_adjusted)\n",
        "        threshold_costs = cost_function(torch.full_like(z_adjusted, self.theta))\n",
        "\n",
        "        # Where current cost is higher than threshold cost, switch to threshold\n",
        "        should_switch_off = above_threshold & (current_costs > threshold_costs)\n",
        "        z_adjusted[should_switch_off] = self.theta\n",
        "\n",
        "        # Check for values below or at threshold that should be switched on\n",
        "        below_or_at_threshold = ~above_threshold\n",
        "        switched_on_values = z_adjusted + self.theta\n",
        "        switched_on_costs = cost_function(switched_on_values)\n",
        "        should_switch_on = below_or_at_threshold & (switched_on_costs < current_costs)\n",
        "        z_adjusted[should_switch_on] = switched_on_values[should_switch_on]\n",
        "\n",
        "        # Apply Heaviside function\n",
        "        z_final = self._heaviside(z_adjusted)\n",
        "\n",
        "        # Reshape back to original shape if needed\n",
        "        if len(original_shape) > 2:\n",
        "            z_final = z_final.view(original_shape)\n",
        "\n",
        "        return z_final\n",
        "\n",
        "    def _calculate_q_l(self, l, t):\n",
        "        \"\"\"\n",
        "        Calculate q_{l,t} based on s_{l,t} and activation threshold.\n",
        "        Following the paper's equation definitions.\n",
        "        \"\"\"\n",
        "        s_l_t = self._calculate_s_l(l, t)\n",
        "\n",
        "        # Make sure the theta multiplication maintains proper dimensions\n",
        "        theta_term = self.theta * self.a[l][t - 1]  # This should broadcast correctly\n",
        "\n",
        "        # Dimensions should match for subtraction\n",
        "        result = s_l_t - theta_term\n",
        "        return result\n",
        "\n",
        "    def _calculate_r_l(self, l, t):\n",
        "        \"\"\"\n",
        "        Calculate r_{l,t} based on z_{l,t} and weight projection.\n",
        "        Following the paper's equation definitions.\n",
        "        \"\"\"\n",
        "\n",
        "        # Handle the case when t is out of bounds\n",
        "        if t >= len(self.a[l-1]):\n",
        "            t = len(self.a[l-1]) - 1\n",
        "\n",
        "        # Get activation from previous layer\n",
        "        a_prev = self.a[l-1][t]  # Shape: [batch_size, n_prev_features]\n",
        "\n",
        "        # Compute W_l @ a_{l-1,t} with proper dimensions\n",
        "        p_l = torch.matmul(a_prev, self.W[l].t())  # Shape: [batch_size, n_features]\n",
        "\n",
        "        # Make sure z has same shape for subtraction\n",
        "        if t < len(self.z[l]):\n",
        "            z_term = self.z[l][t]\n",
        "        else:\n",
        "            # If t is beyond z's length, use the last timestep\n",
        "            z_term = self.z[l][-1]\n",
        "\n",
        "        # Compute result with proper dimensions\n",
        "        result = -p_l + z_term\n",
        "        return result\n",
        "\n",
        "    def _calculate_s_l(self, l, t):\n",
        "        \"\"\"\n",
        "        Calculate s_{l,t} for intermediate updates.\n",
        "        Following the paper's equation definitions.\n",
        "        \"\"\"\n",
        "\n",
        "        # Handle the case when t is out of bounds\n",
        "        if t >= len(self.a[l-1]):\n",
        "            t = len(self.a[l-1]) - 1\n",
        "\n",
        "        # Get activation from previous layer\n",
        "        a_prev = self.a[l-1][t]  # Shape: [batch_size, n_prev_features]\n",
        "\n",
        "        # Compute W_l @ a_{l-1,t} with proper dimensions\n",
        "        p_l = torch.matmul(a_prev, self.W[l].t())  # Shape: [batch_size, n_features]\n",
        "\n",
        "        if t > 1 and t-1 < len(self.z[l]):\n",
        "            # Add delta * z_{l,t-1} term if t > 1\n",
        "            z_prev = self.z[l][t-1]\n",
        "            result = p_l + self.delta * z_prev\n",
        "        else:\n",
        "            result = p_l\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _calculate_s_L(self):\n",
        "        \"\"\"\n",
        "        Calculate s_L for the last layer update.\n",
        "        Following the paper's equation definitions.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get activation from second-to-last layer at time T\n",
        "        a_Lminus1_T = self.a[self.L-2][self.T-1]  # Shape: [batch_size, n_prev_features]\n",
        "\n",
        "        # Compute W_L @ a_{L-1,T} with proper dimensions\n",
        "        result = torch.matmul(a_Lminus1_T, self.W[self.L-1].t())  # Shape: [batch_size, n_features]\n",
        "        return result\n",
        "\n",
        "    # ============ a_{l,t} update functions ============\n",
        "    # Activation update for l=1,...,L-2, t=1,...,T-1 (Equation 8)\n",
        "    def _activation_update(self, u_l, w_l, v_l, t):\n",
        "        \"\"\"\n",
        "        Update activations based on equation (8) in the paper:\n",
        "        For l=1,...,L-2, t=1,...,T-1\n",
        "        \"\"\"\n",
        "        alpha = self.rho / 2\n",
        "        beta = alpha\n",
        "        batch_size = u_l.shape[0]\n",
        "        n_features = u_l.shape[1]\n",
        "        next_layer_idx = min(t + 1, len(self.W)-1)\n",
        "        W_next = self.W[next_layer_idx]\n",
        "\n",
        "        # Calculate θ²I term\n",
        "        theta_squared_term = (self.delta**2 * alpha * torch.eye(n_features, device=self.device))\n",
        "\n",
        "        # Calculate W_{l+1}^T W_{l+1} term\n",
        "        w_term = alpha * (W_next.t() @ W_next)\n",
        "\n",
        "        # Combine terms\n",
        "        term1 = theta_squared_term + w_term + beta * torch.eye(n_features, device=self.device)\n",
        "        # Initialize right-hand side term\n",
        "        term2 = torch.zeros((n_features, batch_size), device=self.device)\n",
        "\n",
        "        # Add -θw_{l,t+1} term\n",
        "        term2 = term2 - self.delta * alpha * w_l.t()\n",
        "\n",
        "        # Add W_{l+1}^T v_{l+1} term\n",
        "        term2 = term2 + alpha * W_next.t() @ v_l.t()\n",
        "\n",
        "        # Add Heaviside term\n",
        "        if t < len(self.z):\n",
        "            z_term = self.z[t] - self.theta\n",
        "            h_term = self._heaviside(z_term)\n",
        "            term2 = term2 + beta * h_term.t()\n",
        "\n",
        "\n",
        "        # Solve the system\n",
        "        update = torch.linalg.solve(term1, term2)\n",
        "\n",
        "        # Transpose back to match expected shape\n",
        "        update = update.t()\n",
        "\n",
        "        return update\n",
        "\n",
        "    # Activation update for l=1,...,L-2 at t=T (Equation 9)\n",
        "    def _activation_update_T(self, u_l, w_l, v_l):\n",
        "        \"\"\"\n",
        "        Update activations for l=1,...,L-2 at t=T based on equation (9)\n",
        "        \"\"\"\n",
        "        alpha = self.rho / 2\n",
        "        beta = alpha\n",
        "        batch_size = u_l.shape[0]\n",
        "        n_features = u_l.shape[1]\n",
        "\n",
        "        # Calculate W_{l+1}^T W_{l+1} term\n",
        "        term1 = alpha * self.W[self.L-2].t() @ self.W[self.L-2]\n",
        "\n",
        "        # Add identity term\n",
        "        term1 = term1 + beta * torch.eye(n_features, device=self.device)\n",
        "\n",
        "        # Initialize right-hand side term\n",
        "        term2 = torch.zeros((batch_size, n_features), device=self.device)\n",
        "\n",
        "        # Add W_{l+1}^T v_{l+1} term\n",
        "        term2 = term2 + alpha * self.W[self.L-2].t() @ v_l\n",
        "\n",
        "        # Add Heaviside term\n",
        "        term2 = term2 + beta * self._heaviside(self.z[self.L-2][self.T] - self.theta)\n",
        "\n",
        "        # Solve the system for each batch\n",
        "        update = torch.solve(term2.t(), term1)[0].t()\n",
        "\n",
        "        return update\n",
        "\n",
        "    # Activation update for l=L-1, t=1,...,T-1 (Equation 10)\n",
        "    def _activation_update_Lminus1(self, u_Lminus1, w_Lminus1, v_Lminus1, t):\n",
        "        \"\"\"\n",
        "        Update activations for l=L-1, t=1,...,T-1 based on equation (10)\n",
        "        \"\"\"\n",
        "        alpha = self.rho / 2\n",
        "        beta = alpha\n",
        "        batch_size = u_Lminus1.shape[0]\n",
        "        n_features = u_Lminus1.shape[1]  # This is 64 (output of previous layer)\n",
        "        W_L = self.W[self.L-1]  # Shape: [64, 128]\n",
        "\n",
        "\n",
        "        # Calculate W_L W_L^T term with correct dimensions\n",
        "        term1 = alpha * (W_L @ W_L.t())  # Shape: [64, 64]\n",
        "\n",
        "        # Add identity term matching W_L dimensions\n",
        "        term1 = term1 + beta * torch.eye(n_features, device=self.device)  # Shape: [64, 64]\n",
        "\n",
        "        # Initialize right-hand side term\n",
        "        term2 = torch.zeros((n_features, batch_size), device=self.device)  # Shape: [64, 128]\n",
        "\n",
        "        # Add W_L^T v_L term - need to transpose v_Lminus1 for correct dimensions\n",
        "        term2 = term2 + alpha * v_Lminus1.t()  # Shape: [64, 128]\n",
        "\n",
        "        # Add Heaviside term\n",
        "        if t < len(self.z[self.L-1]):\n",
        "            z_term = self.z[self.L-1][t] - self.theta\n",
        "            h_term = self._heaviside(z_term)\n",
        "            term2 = term2 + beta * h_term.t()\n",
        "\n",
        "        # Add Lagrange multiplier term only at final timestep\n",
        "        if t == self.T:\n",
        "            term2 = term2 + W_L.t() @ self.lambda_lagrange.t()\n",
        "\n",
        "\n",
        "        # Solve the system\n",
        "        # term1: [64, 64], term2: [64, 128]\n",
        "        update = torch.linalg.solve(term1, term2)  # Shape: [64, 128]\n",
        "\n",
        "        # Transpose back to match expected shape\n",
        "        update = update.t()  # Shape: [128, 64]\n",
        "\n",
        "        return update\n",
        "\n",
        "    # Activation update for l=L-1, t=T (Equation 11)\n",
        "    def _activation_update_Lminus1_T(self, u_Lminus1, w_Lminus1, v_Lminus1):\n",
        "        \"\"\"\n",
        "        Update activations for l=L-1, t=T based on equation (11)\n",
        "        Following equation: a_{L-1,T} = (W_L^T W_L + I)^{-1} [W_L^T (αv_L - λ/2) + β h(z_{L-1,T} - θ)]\n",
        "        \"\"\"\n",
        "        alpha = self.rho / 2\n",
        "        beta = alpha\n",
        "        batch_size = u_Lminus1.shape[0]  # 128\n",
        "        n_features = u_Lminus1.shape[1]  # 64\n",
        "        W_L = self.W[self.L-1]  # Shape: [64, 128]\n",
        "\n",
        "\n",
        "        # Calculate W_L W_L^T term with correct dimensions\n",
        "        term1 = alpha * (W_L @ W_L.t())  # Shape: [64, 64]\n",
        "\n",
        "        # Add identity term matching W_L dimensions\n",
        "        term1 = term1 + beta * torch.eye(n_features, device=self.device)  # Shape: [64, 64]\n",
        "\n",
        "        # Initialize right-hand side term\n",
        "        term2 = torch.zeros((n_features, batch_size), device=self.device)  # Shape: [64, 128]\n",
        "\n",
        "        # Add W_L^T v_L term - need to transpose v_Lminus1 for correct dimensions\n",
        "        term2 = term2 + alpha * v_Lminus1.t()  # Shape: [64, 128]\n",
        "\n",
        "        # Add Heaviside term for final timestep\n",
        "        z_term = self.z[self.L-1][self.T-1] - self.theta  # Use T-1 for indexing\n",
        "        h_term = self._heaviside(z_term)\n",
        "        term2 = term2 + beta * h_term.t()  # Shape: [64, 128]\n",
        "\n",
        "        # Add Lagrange multiplier term for final timestep with proper dimensions\n",
        "        # lambda_lagrange: [128, 10]\n",
        "        # First reshape lambda to match batch dimension: [128, 64]\n",
        "        lambda_reshaped = self.lambda_lagrange @ W_L[:, :10].t()  # [128, 64]\n",
        "        term2 = term2 - (lambda_reshaped.t() / 2)  # [64, 128]\n",
        "\n",
        "\n",
        "        # Solve the system\n",
        "        # term1: [64, 64], term2: [64, 128]\n",
        "        update = torch.linalg.solve(term1, term2)  # Shape: [64, 128]\n",
        "\n",
        "        # Transpose back to match expected shape\n",
        "        update = update.t()  # Shape: [128, 64]\n",
        "\n",
        "        return update\n",
        "    # ============ lagrange multiplier update ============\n",
        "\n",
        "    def _lambda_update(self, y):\n",
        "        \"\"\"\n",
        "        Update the Lagrange multiplier lambda.\n",
        "\n",
        "        Parameters:\n",
        "        - y (torch.Tensor): Target values [num_classes, batch_size]\n",
        "\n",
        "        Returns:\n",
        "        - Updated lambda [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        # Get last timestep activations: [batch_size, hidden_dim]\n",
        "        last_layer_activations = self.a[self.L - 2][self.T-1]\n",
        "\n",
        "        # Get last timestep outputs: [batch_size, num_classes]\n",
        "        last_layer_outputs = self.z[self.L - 1][self.T-1]\n",
        "\n",
        "        # Compute W @ a term: [batch_size, num_classes]\n",
        "        # W: [num_classes, hidden_dim], a: [batch_size, hidden_dim]\n",
        "        Wa_term = last_layer_activations @ self.W[self.L - 1].t()\n",
        "\n",
        "        # Compute rho term: [batch_size, num_classes]\n",
        "        rho_term = self.rho * (last_layer_outputs - Wa_term)\n",
        "\n",
        "        # Ensure y has shape [batch_size, num_classes]\n",
        "        if y.shape[0] != last_layer_outputs.shape[0]:\n",
        "            y = y.t()\n",
        "\n",
        "        # Compute y term: [batch_size, num_classes]\n",
        "        y_term = y - last_layer_outputs\n",
        "\n",
        "        # Update lambda\n",
        "        self.lambda_lagrange = self.lambda_lagrange + rho_term + y_term\n",
        "\n",
        "        return self.lambda_lagrange\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Process N-MNIST inputs through LIF layers and get final predictions.\n",
        "\n",
        "        Parameters:\n",
        "        - inputs (torch.Tensor): Shape [timesteps, in_features, batch_size]\n",
        "\n",
        "        Returns:\n",
        "        - Final layer membrane potentials for classification\n",
        "        \"\"\"\n",
        "        timesteps, in_features, batch_size = inputs.shape\n",
        "\n",
        "        # Initialize membrane potentials\n",
        "        mem = []\n",
        "        for layer_dim in [self.W[i].shape[0] for i in range(len(self.W))]:\n",
        "            mem.append(torch.zeros(batch_size, layer_dim, device=self.device))\n",
        "\n",
        "        # Create LIF neurons\n",
        "        neurons = []\n",
        "        for l in range(len(self.W)):\n",
        "            if l < len(self.W) - 1:\n",
        "                # Hidden layers with reset\n",
        "                lif = snn.Leaky(\n",
        "                    beta=self.delta,\n",
        "                    threshold=self.theta,\n",
        "                    reset_mechanism=\"subtract\",\n",
        "                    learn_beta=False,\n",
        "                    learn_threshold=False\n",
        "                )\n",
        "            else:\n",
        "                # Output layer accumulates without reset\n",
        "                lif = snn.Leaky(\n",
        "                    beta=self.delta,\n",
        "                    threshold=self.theta,\n",
        "                    reset_mechanism=\"none\",\n",
        "                    learn_beta=False,\n",
        "                    learn_threshold=False\n",
        "                )\n",
        "            neurons.append(lif)\n",
        "\n",
        "        # Process each timestep\n",
        "        for t in range(timesteps):\n",
        "            x = inputs[t].T  # [batch_size, in_features]\n",
        "\n",
        "            # Process through each layer\n",
        "            for l in range(len(self.W)):\n",
        "                # Linear transformation\n",
        "                x = x @ self.W[l].T\n",
        "\n",
        "                # Apply LIF neuron dynamics\n",
        "                spk, mem[l] = neurons[l](x, mem[l])\n",
        "\n",
        "                # Output spikes become input to next layer\n",
        "                x = spk\n",
        "\n",
        "        # Return final layer membrane potentials for classification\n",
        "        return mem[-1]  # Shape: [batch_size, n_outputs]\n",
        "    def calculate_loss(self, raw_predictions, targets):\n",
        "        \"\"\"\n",
        "        Calculate Cross Entropy Loss\n",
        "\n",
        "        Parameters:\n",
        "        - raw_predictions (torch.Tensor): Output activities [batch_size, num_classes]\n",
        "        - targets (torch.Tensor): Target values as class indices [batch_size]\n",
        "        \"\"\"\n",
        "        # Convert one-hot encoded targets to class indices if necessary\n",
        "        if targets.shape[0] != raw_predictions.shape[0]:\n",
        "            targets = targets.t()\n",
        "        if targets.dim() > 1:\n",
        "            targets = targets.argmax(dim=1)\n",
        "\n",
        "        # Calculate cross-entropy loss\n",
        "        loss = self.loss_fn(raw_predictions, targets)\n",
        "\n",
        "        return loss\n",
        "    def evaluate(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Evaluate N-MNIST specific metrics based on class membership\n",
        "        \"\"\"\n",
        "        print(\"\\n=== N-MNIST Analysis ===\")\n",
        "\n",
        "        # Get final layer activity\n",
        "        raw_predictions = self.feed_forward(inputs)  # [batch_size, 10]\n",
        "        print(\"\\nFinal Layer Activity:\")\n",
        "        print(f\"Shape: {raw_predictions.shape}\")\n",
        "        print(raw_predictions)\n",
        "        print(f\"Activity Stats - Mean: {raw_predictions.mean():.4f}, Std: {raw_predictions.std():.4f}\")\n",
        "        print(f\"Activity Range - Min: {raw_predictions.min():.4f}, Max: {raw_predictions.max():.4f}\")\n",
        "\n",
        "        # Threshold-based class assignments\n",
        "        threshold = self.theta\n",
        "        class_assignments = (raw_predictions > threshold).float()\n",
        "\n",
        "        # Get predicted classes and actual classes\n",
        "        pred_classes = raw_predictions.argmax(dim=1)\n",
        "        true_classes = targets.t().argmax(dim=1)\n",
        "\n",
        "        # Initialize metrics storage\n",
        "        metrics_per_class = []\n",
        "\n",
        "        print(\"\\nConfusion Matrix and Metrics:\")\n",
        "        for class_idx in range(10):\n",
        "            # Get predictions for each true class\n",
        "            predicted_for_class = pred_classes[true_classes == class_idx]\n",
        "\n",
        "            # Confusion matrix row for true class `class_idx`\n",
        "            predictions_count = [(predicted_for_class == pred_class).sum().item() for pred_class in range(10)]\n",
        "            print(f\"Real Class {class_idx}: [{', '.join(map(str, predictions_count))}]\")\n",
        "\n",
        "            # Calculate True Positives, False Positives, False Negatives\n",
        "            TP = (pred_classes == true_classes) & (true_classes == class_idx)\n",
        "            FP = (pred_classes == class_idx) & (true_classes != class_idx)\n",
        "            FN = (pred_classes != class_idx) & (true_classes == class_idx)\n",
        "\n",
        "            TP_count = TP.sum().float()\n",
        "            FP_count = FP.sum().float()\n",
        "            FN_count = FN.sum().float()\n",
        "\n",
        "            # Calculate precision, recall, F1 for each class\n",
        "            precision = TP_count / (TP_count + FP_count) if TP_count + FP_count > 0 else torch.tensor(0.0)\n",
        "            recall = TP_count / (TP_count + FN_count) if TP_count + FN_count > 0 else torch.tensor(0.0)\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else torch.tensor(0.0)\n",
        "\n",
        "            # Print metrics in one line for each class\n",
        "            print(f\"Class {class_idx} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "            # Store metrics for macro-averaging\n",
        "            metrics_per_class.append({'precision': precision, 'recall': recall, 'f1': f1})\n",
        "\n",
        "        # Calculate macro-averaged metrics\n",
        "        macro_precision = torch.stack([m['precision'] for m in metrics_per_class]).mean()\n",
        "        macro_recall = torch.stack([m['recall'] for m in metrics_per_class]).mean()\n",
        "        macro_f1 = torch.stack([m['f1'] for m in metrics_per_class]).mean()\n",
        "        accuracy = (pred_classes == true_classes).float().mean()\n",
        "\n",
        "        print(\"\\nOverall Metrics:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Macro Precision: {macro_precision:.4f}\")\n",
        "        print(f\"Macro Recall: {macro_recall:.4f}\")\n",
        "        print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
        "\n",
        "        # Calculate firing accuracy\n",
        "        firing_correct = class_assignments[range(len(true_classes)), true_classes].sum()\n",
        "        firing_accuracy = firing_correct / len(true_classes)\n",
        "        print(f\"\\nFiring Accuracy: {firing_accuracy:.4f}\")\n",
        "\n",
        "        # Calculate and print loss\n",
        "        loss = self.calculate_loss(raw_predictions, targets)\n",
        "        print(f\"\\nLoss: {loss.item():.6f}\")\n",
        "\n",
        "        return loss, raw_predictions\n",
        "\n",
        "    def fit(self, inputs, targets):\n",
        "        #print(\"\\n=== Starting Warming Phase ===\")\n",
        "        #print(\"\\nInitial state:\")\n",
        "        #print(f\"Weight samples (first layer): {self.W[0][0, :5]}\")\n",
        "        #print(f\"Lambda sample: {self.lambda_lagrange[0, :5]}\")\n",
        "        #print(f\"z values sample (first layer): {self.z[0][0, 0, :5]}\")\n",
        "        #print(f\"a values sample (first layer): {self.a[0][0, 0, :5]}\")\n",
        "        for l in range(1, self.L):\n",
        "            #print(f\"\\n--- Layer {l} Updates ---\")\n",
        "            # Update self.W[l] using the function _weight_update\n",
        "            self.W[l] = self._weight_update(self.z[l], self.a[l - 1])\n",
        "\n",
        "            for t in range(1, self.T-1):  # Changed from self.T to self.T-1\n",
        "                if l < self.L - 1:\n",
        "                    u_l, w_l, v_l = self._calculate_u_w_v(l, t)\n",
        "                    self.a[l][t] = self._activation_update(u_l, w_l, v_l, t)\n",
        "                else:\n",
        "                    u_Lminus1, w_Lminus1, v_Lminus1 = self._calculate_u_w_v(l, t)\n",
        "                    self.a[l][t] = self._activation_update_Lminus1(u_Lminus1, w_Lminus1, v_Lminus1, t)\n",
        "\n",
        "                # update self.z[l][t] using the function _z_update and check_entries\n",
        "                self.z[l][t] = self._z_update(l, t)\n",
        "                self.z[l][t] = self.check_entries(self.z[l][t], cost_function=lambda z: torch.norm(z))\n",
        "            #print(f\"\\nLayer {l} final stats:\")\n",
        "            #print(f\"z mean: {self.z[l].mean():.4f}, max: {self.z[l].max():.4f}, min: {self.z[l].min():.4f}\")\n",
        "            #print(f\"a mean: {self.a[l].mean():.4f}, max: {self.a[l].max():.4f}, min: {self.a[l].min():.4f}\")\n",
        "\n",
        "            # Handle the final timestep separately\n",
        "            t = self.T - 1  # Use last valid index\n",
        "            if l < self.L - 1:\n",
        "                u_l, w_l, v_l = self._calculate_u_w_v(l, t)\n",
        "                self.a[l][t] = self._activation_update_T(u_l, w_l, v_l)\n",
        "            else:\n",
        "                u_Lminus1, w_Lminus1, v_Lminus1 = self._calculate_u_w_v(l, t)\n",
        "                self.a[l][t] = self._activation_update_Lminus1_T(u_Lminus1, w_Lminus1, v_Lminus1)\n",
        "\n",
        "            if l < self.L - 1:\n",
        "                self.z[l][t] = self._z_update_T(l)\n",
        "                self.z[l][t] = self.check_entries(self.z[l][t], cost_function=lambda z: torch.norm(z))\n",
        "            else:\n",
        "                self.z[l][t] = self._z_update_L(targets)\n",
        "                self.z[l][t] = self.check_entries(self.z[l][t], cost_function=lambda z: torch.norm(z))\n",
        "\n",
        "        # ----- Update the last layer -----\n",
        "        self.W[self.L - 1] = self._weight_update_L(self.z[self.L - 1], self.a[self.L - 2], targets)\n",
        "\n",
        "        for t in range(1, self.T-1):  # Changed from self.T to self.T-1\n",
        "            self.z[self.L - 1][t] = self._z_update_L(targets)\n",
        "            self.z[self.L - 1][t] = self.check_entries(self.z[self.L - 1][t], cost_function=lambda z: torch.norm(z))\n",
        "\n",
        "        # Handle final timestep of last layer\n",
        "        t = self.T - 1  # Use last valid index\n",
        "        self.z[self.L - 1][t] = self._z_update_L_T(targets)\n",
        "        self.z[self.L - 1][t] = self.check_entries(self.z[self.L - 1][t], cost_function=lambda z: torch.norm(z))\n",
        "\n",
        "        # Update the lagrange multiplier using the function _lambda_update\n",
        "        self.lambda_lagrange = self._lambda_update(targets)\n",
        "        loss, predictions = self.evaluate(inputs, targets)\n",
        "\n",
        "        print(f\"\\Training phase interim metrics:\")\n",
        "        print(f\"  Loss: {loss:.6f}\")\n",
        "\n",
        "\n",
        "        return loss, predictions\n",
        "\n",
        "    def warming(self, inputs, targets):\n",
        "        #print(\"\\n=== Starting Warming Phase ===\")\n",
        "        #print(\"\\nInitial state:\")\n",
        "        #print(f\"Weight samples (first layer): {self.W[0][0, :5]}\")\n",
        "        #print(f\"Lambda sample: {self.lambda_lagrange[0, :5]}\")\n",
        "        #print(f\"z values sample (first layer): {self.z[0][0, 0, :5]}\")\n",
        "        #print(f\"a values sample (first layer): {self.a[0][0, 0, :5]}\")\n",
        "        for l in range(1, self.L):\n",
        "            #print(f\"\\n--- Layer {l} Updates ---\")\n",
        "            # Update self.W[l] using the function _weight_update\n",
        "            self.W[l] = self._weight_update(self.z[l], self.a[l - 1])\n",
        "\n",
        "            for t in range(1, self.T-1):  # Changed from self.T to self.T-1\n",
        "                if l < self.L - 1:\n",
        "                    u_l, w_l, v_l = self._calculate_u_w_v(l, t)\n",
        "                    self.a[l][t] = self._activation_update(u_l, w_l, v_l, t)\n",
        "                else:\n",
        "                    u_Lminus1, w_Lminus1, v_Lminus1 = self._calculate_u_w_v(l, t)\n",
        "                    self.a[l][t] = self._activation_update_Lminus1(u_Lminus1, w_Lminus1, v_Lminus1, t)\n",
        "\n",
        "                # update self.z[l][t] using the function _z_update and check_entries\n",
        "                self.z[l][t] = self._z_update(l, t)\n",
        "                self.z[l][t] = self.check_entries(self.z[l][t], cost_function=lambda z: torch.norm(z))\n",
        "            #print(f\"\\nLayer {l} final stats:\")\n",
        "            #print(f\"z mean: {self.z[l].mean():.4f}, max: {self.z[l].max():.4f}, min: {self.z[l].min():.4f}\")\n",
        "            #print(f\"a mean: {self.a[l].mean():.4f}, max: {self.a[l].max():.4f}, min: {self.a[l].min():.4f}\")\n",
        "\n",
        "            # Handle the final timestep separately\n",
        "            t = self.T - 1  # Use last valid index\n",
        "            if l < self.L - 1:\n",
        "                u_l, w_l, v_l = self._calculate_u_w_v(l, t)\n",
        "                self.a[l][t] = self._activation_update_T(u_l, w_l, v_l)\n",
        "            else:\n",
        "                u_Lminus1, w_Lminus1, v_Lminus1 = self._calculate_u_w_v(l, t)\n",
        "                self.a[l][t] = self._activation_update_Lminus1_T(u_Lminus1, w_Lminus1, v_Lminus1)\n",
        "\n",
        "            if l < self.L - 1:\n",
        "                self.z[l][t] = self._z_update_T(l)\n",
        "                self.z[l][t] = self.check_entries(self.z[l][t], cost_function=lambda z: torch.norm(z))\n",
        "            else:\n",
        "                self.z[l][t] = self._z_update_L(targets)\n",
        "                self.z[l][t] = self.check_entries(self.z[l][t], cost_function=lambda z: torch.norm(z))\n",
        "\n",
        "        # ----- Update the last layer -----\n",
        "        self.W[self.L - 1] = self._weight_update_L(self.z[self.L - 1], self.a[self.L - 2], targets)\n",
        "\n",
        "        for t in range(1, self.T-1):  # Changed from self.T to self.T-1\n",
        "            self.z[self.L - 1][t] = self._z_update_L(targets)\n",
        "            self.z[self.L - 1][t] = self.check_entries(self.z[self.L - 1][t], cost_function=lambda z: torch.norm(z))\n",
        "\n",
        "        # Handle final timestep of last layer\n",
        "        t = self.T - 1  # Use last valid index\n",
        "        self.z[self.L - 1][t] = self._z_update_L_T(targets)\n",
        "        self.z[self.L - 1][t] = self.check_entries(self.z[self.L - 1][t], cost_function=lambda z: torch.norm(z))\n",
        "\n",
        "        loss, predictions = self.evaluate(inputs, targets)\n",
        "\n",
        "        print(\"\\n=== Final State ===\")\n",
        "        print(f\"Final weight samples: {self.W[-1][0, :5]}\")\n",
        "        print(f\"Final lambda sample: {self.lambda_lagrange[0, :5]}\")\n",
        "        print(f\"\\nWarming phase interim metrics:\")\n",
        "        print(f\"  Loss: {loss:.6f}\")\n",
        "\n",
        "        return loss, predictions\n",
        "\n",
        "\n",
        "    def _calculate_u_w_v(self, l, t):\n",
        "        \"\"\"\n",
        "        Calculate vectors based on equations in Section 2.2 of the paper:\n",
        "        u_l = [z_{l,1}, z_{l,2} - δz_{l,1}, ..., z_{l,T} - δz_{l,T-1}]\n",
        "        v_l = u_l + ϑ[0, a_{l,1}, ..., a_{l,T-1}]\n",
        "        w_l = u_l - W_l[a_{l-1,1}, ..., a_{l-1,T}]\n",
        "\n",
        "        Parameters:\n",
        "        - l (int): Layer index\n",
        "        - t (int): Time step\n",
        "\n",
        "        Returns:\n",
        "        - u_l, w_l, v_l (torch.Tensor): Calculated vectors with shape [batch_size, n_features]\n",
        "        \"\"\"\n",
        "        batch_size = self.z[l].shape[1]\n",
        "        n_features = self.z[l].shape[2]\n",
        "        device = self.device\n",
        "\n",
        "        # Calculate u_l\n",
        "        if t == 0:  # First timestep\n",
        "            u_l = self.z[l][0]  # Shape: [batch_size, n_features]\n",
        "        else:\n",
        "            # z_{l,t} - δz_{l,t-1}\n",
        "            u_l = self.z[l][t] - self.delta * self.z[l][t-1]  # Shape: [batch_size, n_features]\n",
        "\n",
        "        # Calculate v_l with proper broadcasting\n",
        "        if t == 0:\n",
        "            v_l = u_l  # No previous activations for t=0\n",
        "        else:\n",
        "            # Add ϑ * previous activation\n",
        "            v_l = u_l.clone()\n",
        "            if t > 1 and t-1 < len(self.a[l]):\n",
        "                v_l = v_l + self.theta * self.a[l][t-1]  # Shape: [batch_size, n_features]\n",
        "\n",
        "        # Calculate w_l\n",
        "        if l > 0:  # Not input layer\n",
        "            # Get previous layer activation\n",
        "            a_prev = self.a[l-1][t]  # Shape: [batch_size, n_prev_features]\n",
        "\n",
        "            # Compute W_l @ a_{l-1,t} with proper dimensions\n",
        "            w_l = u_l - torch.matmul(a_prev, self.W[l].t())  # Shape: [batch_size, n_features]\n",
        "        else:\n",
        "            w_l = u_l\n",
        "\n",
        "\n",
        "        return u_l, v_l, w_l\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import tonic\n",
        "import tonic.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tonic import DiskCachedDataset\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations\n",
        "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
        "frame_transform = transforms.Compose([\n",
        "    transforms.Denoise(filter_time=10000),\n",
        "    transforms.ToFrame(sensor_size=sensor_size, time_window=1000)\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
        "testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYuF2XdhGXZg",
        "outputId": "5a36d8f8-4d58-4110-ed23-99bdc23b7077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes in __init__:\n",
            "W[0] shape: torch.Size([100, 1156])\n",
            "W[1] shape: torch.Size([50, 100])\n",
            "z[0] shape: torch.Size([300, 128, 100])\n",
            "z[1] shape: torch.Size([300, 128, 50])\n",
            "a[0] shape: torch.Size([300, 128, 100])\n",
            "a[1] shape: torch.Size([300, 128, 50])\n",
            "Sample data shape: torch.Size([128, 309, 2, 34, 34])\n",
            "Sample target shape: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_nmnist_data(inputs, labels, device, n_timesteps=100):\n",
        "    \"\"\"\n",
        "    Modified data preparation with consistent device placement.\n",
        "    \"\"\"\n",
        "    batch_size = inputs.shape[0]\n",
        "\n",
        "    # Move inputs to device first\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "    # Reshape inputs to [time, features, batch]\n",
        "    inputs = inputs.float().reshape(batch_size, -1, sensor_size[0] * sensor_size[1])\n",
        "    inputs = inputs.permute(1, 2, 0).contiguous()\n",
        "\n",
        "    # Handle timesteps\n",
        "    if inputs.shape[0] > n_timesteps:\n",
        "        inputs = inputs[:n_timesteps]\n",
        "    elif inputs.shape[0] < n_timesteps:\n",
        "        padding = torch.zeros((n_timesteps - inputs.shape[0], inputs.shape[1], batch_size),\n",
        "                            device=device)\n",
        "        inputs = torch.cat((inputs, padding), dim=0)\n",
        "\n",
        "    # One-hot encode labels (now labels are already on correct device)\n",
        "    labels_onehot = torch.zeros((10, batch_size), device=device)\n",
        "    labels_onehot.scatter_(0, labels.unsqueeze(0), 1)\n",
        "\n",
        "    return inputs, labels_onehot\n",
        "\n",
        "def train(model, trainloader, num_epochs):\n",
        "\n",
        "    # Warming phase\n",
        "    print(\"\\nWarming Phase:\")\n",
        "    warming_losses = []\n",
        "    for epoch in range(5):\n",
        "        print(f'Warming Epoch [{epoch+1}/2]')\n",
        "        epoch_losses = []\n",
        "        epoch_accuracies = []\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Prepare data\n",
        "            inputs, labels = prepare_nmnist_data(inputs, labels, device)\n",
        "\n",
        "            # Warming step\n",
        "            loss, predictions = model.warming(inputs, labels)\n",
        "\n",
        "            # Ensure predictions and labels have correct shape\n",
        "            # predictions: [batch_size, num_classes]\n",
        "            # labels: [num_classes, batch_size] -> [batch_size, num_classes]\n",
        "            labels = labels.t()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_classes = torch.argmax(predictions, dim=1)  # [batch_size]\n",
        "            true_classes = torch.argmax(labels, dim=1)      # [batch_size]\n",
        "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
        "\n",
        "            epoch_losses.append(loss)\n",
        "            epoch_accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "        # Epoch summary\n",
        "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        avg_acc = sum(epoch_accuracies) / len(epoch_accuracies)\n",
        "        warming_losses.append(avg_loss)\n",
        "        print(f'  Epoch Summary - Loss: {avg_loss:.6f}, Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    # Main training\n",
        "    print(\"\\nMain Training Phase:\")\n",
        "    training_metrics = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        epoch_losses = []\n",
        "        epoch_accuracies = []\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "            print(f'  Batch [{batch_idx+1}/{len(trainloader)}]')\n",
        "\n",
        "            # Prepare data\n",
        "            inputs, labels = prepare_nmnist_data(inputs, labels, device)\n",
        "\n",
        "            # Training step\n",
        "            loss, predictions = model.fit(inputs, labels)\n",
        "\n",
        "            # Ensure predictions and labels have correct shape\n",
        "            labels = labels.t()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_classes = torch.argmax(predictions, dim=1)\n",
        "            true_classes = torch.argmax(labels, dim=1)\n",
        "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
        "\n",
        "            epoch_losses.append(loss)\n",
        "            epoch_accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "        # Epoch summary\n",
        "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        avg_acc = sum(epoch_accuracies) / len(epoch_accuracies)\n",
        "        training_metrics.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'loss': avg_loss,\n",
        "            'accuracy': avg_acc\n",
        "        })\n",
        "        print(f'  Epoch Summary - Loss: {avg_loss:.6f}, Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    return warming_losses, training_metrics\n",
        "\n",
        "def evaluate(model, testloader):\n",
        "    print(\"\\nEvaluation Phase:\")\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    num_batches = 0\n",
        "    batch_metrics = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(testloader):\n",
        "            print(f'  Batch [{batch_idx+1}/{len(testloader)}]')\n",
        "\n",
        "            # Prepare data\n",
        "            inputs, labels = prepare_nmnist_data(inputs, labels, device)\n",
        "\n",
        "            # Forward pass\n",
        "            loss, predictions = model.evaluate(inputs, labels)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            pred_classes = torch.argmax(predictions, dim=0)\n",
        "            true_classes = torch.argmax(labels, dim=0)\n",
        "            accuracy = (pred_classes == true_classes).float().mean().item()\n",
        "\n",
        "            # Store batch metrics\n",
        "            batch_metrics.append({\n",
        "                'batch': batch_idx + 1,\n",
        "                'loss': loss,\n",
        "                'accuracy': accuracy\n",
        "            })\n",
        "\n",
        "            total_loss += loss\n",
        "            total_acc += accuracy\n",
        "            num_batches += 1\n",
        "\n",
        "            print(f'    Loss: {loss:.6f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_acc = total_acc / num_batches\n",
        "\n",
        "    print(f'\\nFinal Evaluation Results:')\n",
        "    print(f'  Average Loss: {avg_loss:.6f}')\n",
        "    print(f'  Average Accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    return avg_loss, avg_acc, batch_metrics\n",
        "\n",
        "# Initialize model and training\n",
        "n_timesteps = 100\n",
        "input_dim = sensor_size[0] * sensor_size[1]\n",
        "hidden_dims = [128, 10]\n",
        "n_outputs = 10\n",
        "# DataLoaders\n",
        "\n",
        "\n",
        "# Calculate size of 20% of data\n",
        "train_size = int(0.5 * len(trainset)) // 256 * 256\n",
        "test_size = int(0.5 * len(testset)) // 256 * 256\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "train_subset = Subset(trainset, torch.randperm(len(trainset))[:train_size])\n",
        "test_subset = Subset(testset, torch.randperm(len(testset))[:test_size])\n",
        "\n",
        "\n",
        "# Create DataLoaders with the subsets\n",
        "batch_size = 256\n",
        "trainloader = DataLoader(train_subset,\n",
        "                        batch_size=batch_size,\n",
        "                        collate_fn=tonic.collation.PadTensors(),\n",
        "                        shuffle=True)\n",
        "testloader = DataLoader(test_subset,\n",
        "                       batch_size=batch_size,\n",
        "                       collate_fn=tonic.collation.PadTensors())\n",
        "\n",
        "print(f\"Original training set size: {len(trainset)}\")\n",
        "print(f\"training set size: {len(train_subset)}\")\n",
        "print(f\"Original test set size: {len(testset)}\")\n",
        "print(f\"test set size: {len(test_subset)}\")\n",
        "\n",
        "# Hyperparameters\n",
        "rho = 0.05\n",
        "delta = torch.tensor(0.7, device=device)\n",
        "theta = torch.tensor(0.15, device=device)\n",
        "\n",
        "# Create model\n",
        "print(\"\\nInitializing model...\")\n",
        "model = ADMM_SNN(\n",
        "    n_samples=batch_size,\n",
        "    n_timesteps=n_timesteps,\n",
        "    input_dim=input_dim,\n",
        "    hidden_dims=hidden_dims,  # Changed final dimension to 10\n",
        "    n_outputs=10,\n",
        "    rho=rho,\n",
        "    delta=delta,\n",
        "    theta=theta\n",
        ")\n",
        "print(model)\n",
        "\n",
        "# Train model\n",
        "print(\"\\nStarting training process...\")\n",
        "num_epochs = 20\n",
        "warming_losses, training_metrics = train(model, trainloader, num_epochs)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_loss, test_acc, test_metrics = evaluate(model, testloader)\n",
        "\n",
        "# Print final results\n",
        "print(\"\\nTraining Summary:\")\n",
        "print(f\"  Warming phase final loss: {warming_losses[-1]:.6f}\")\n",
        "print(f\"  Training final loss: {training_metrics[-1]['loss']:.6f}\")\n",
        "print(f\"  Training final accuracy: {training_metrics[-1]['accuracy']:.4f}\")\n",
        "print(\"\\nTest Results:\")\n",
        "print(f\"  Test Loss: {test_loss:.6f}\")\n",
        "print(f\"  Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "x62OR-ppGmCD",
        "outputId": "bec5935c-5516-4522-b54d-c9fa794c3390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes in __init__:\n",
            "W[0] shape: torch.Size([128, 1156])\n",
            "W[1] shape: torch.Size([64, 128])\n",
            "z[0] shape: torch.Size([300, 128, 128])\n",
            "z[1] shape: torch.Size([300, 128, 64])\n",
            "a[0] shape: torch.Size([300, 128, 128])\n",
            "a[1] shape: torch.Size([300, 128, 64])\n",
            "Sample data shape: torch.Size([128, 310, 2, 34, 34])\n",
            "Sample target shape: torch.Size([128])\n",
            "------ Warming Epoch: 0 ------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (128x64 and 128x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-a6859a70aea8>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarming_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Warming completed on batch {i+1}/{len(trainloader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Run warming on only one batch for simplicity; you could extend it if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-9bd86018f46a>\u001b[0m in \u001b[0;36mwarming\u001b[0;34m(self, inputs, labels, epochs, beta, gamma)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;31m# Update weights W_l using the weight update function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0;31m# Update activations (a) and membrane potentials (z) for each time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-9bd86018f46a>\u001b[0m in \u001b[0;36m_weight_update\u001b[0;34m(self, layer_output, activation_input)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mrho_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-9bd86018f46a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mrho_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_half\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mactivation_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x64 and 128x128)"
          ]
        }
      ]
    }
  ]
}