{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp6RGuRI5N9f00P98U5jSF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ergysmedaunipd/thesis/blob/main/THESIS_SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiTDF-_1TrNZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class ADMM_SNN:\n",
        "    \"\"\" Class for ADMM Neural Network. \"\"\"\n",
        "\n",
        "    def __init__(self, n_samples: int, n_timesteps: int, input_dim: int, hidden_dims: List[int], n_outputs: int, rho: float, deltas: torch.Tensor, thetas: torch.Tensor):\n",
        "        self.device = \"cpu\"\n",
        "\n",
        "        # Define hyperparameters:\n",
        "        # - thetas = Thresholds (can be all the same or different for each neuron)\n",
        "        # - deltas = Decay factors (can be all the same or different for each neuron)\n",
        "        # - roh = Penalty parameter. All the \\alpha_{l,t} = \\beta_{l,t} = \\rho/2\n",
        "\n",
        "        self.rho = rho\n",
        "        self.deltas = deltas\n",
        "        self.thetas = thetas\n",
        "\n",
        "        self.L = len(hidden_dims)\n",
        "        self.T = n_timesteps\n",
        "\n",
        "        # Define a_0_t, which will be the input to the first layer\n",
        "        self.a0 = torch.zeros(\n",
        "            (n_timesteps, n_samples, input_dim)).to(self.device)\n",
        "\n",
        "        # === Initialize W_l ===\n",
        "        self.W = []\n",
        "\n",
        "        # Now define the weights for each layer\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            if i == 0:\n",
        "                self.W.append(torch.zeros(\n",
        "                    (hidden_dim, input_dim)).to(self.device))\n",
        "            else:\n",
        "                self.W.append(torch.zeros(\n",
        "                    (hidden_dim, hidden_dims[i-1])).to(self.device))\n",
        "\n",
        "        # === Initialize z_l ===\n",
        "        self.z = []\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            self.z.append(torch.zeros(\n",
        "                (n_timesteps, n_samples, hidden_dim)).to(self.device))\n",
        "\n",
        "        # === Initialize a_l ===\n",
        "        self.a = []\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            self.a.append(torch.zeros(\n",
        "                (n_timesteps, n_samples, hidden_dim)).to(self.device))\n",
        "\n",
        "        # Check how to make initialization?\n",
        "        self.lambda_lagrange = torch.zeros(\n",
        "            (n_samples, n_outputs)).to(self.device)\n",
        "\n",
        "    def _heaviside(self, x):\n",
        "        # Implement the heaviside function that takes in input some vector of z and returns 0 or 1, based on the thresholds self.thetas\n",
        "        return\n",
        "\n",
        "    # ============ W_{l} update functions ============\n",
        "    def _weight_update(self, layer_output, activation_input):\n",
        "        # Implement the Weight update of layers 1, ..., L-1, i.e. line 2 of Algorithm 2 (Equation (4) where \\alpha_{l,t} = \\rho/2 [same consideration applies everywhere below])\n",
        "        return\n",
        "\n",
        "    def _weight_update_L(self, layer_output, activation_input):\n",
        "        # Define auxiliary variable x_L ... i.e. line 10 of Algorithm 2 (Equation (6))\n",
        "        return\n",
        "\n",
        "    # ============ z_{l,t} update functions ============\n",
        "    def _z_update(self, _arguments_here):\n",
        "        # First argument in Line 5 of Algorithm 2 (Equation (14))\n",
        "        return\n",
        "\n",
        "    def _z_update_T(self, _arguments_here):\n",
        "        # First argument in Line 8 of Algorithm 2 (Equation (14)*)\n",
        "        return\n",
        "\n",
        "    def _z_update_L(self, _arguments_here):\n",
        "        # Line 12 (Equation (16))\n",
        "        return\n",
        "\n",
        "    def _z_update_L_T(self, _arguments_here):\n",
        "        # Line 14 (Equation (16)*)\n",
        "        return\n",
        "\n",
        "    def check_entries(self, z, cost_function):\n",
        "        # Implements algorithm 1, used in lines 5 and 8 of Algorithm 2\n",
        "        return\n",
        "\n",
        "    # ============ a_{l,t} update functions ============\n",
        "    def _activation_update(self, _arguments_here):\n",
        "        # Implement the Activation update for l=1,...,L-2, t=1,...,T-1 (line 4)\n",
        "        return\n",
        "\n",
        "    def _activation_update_T(self, _arguments_here):\n",
        "        # Implement the Activation update for l=1,...,L-2, t=T, (line 7)\n",
        "        return\n",
        "\n",
        "    def _activation_update_Lminus1(self, _arguments_here):\n",
        "        # Implement the Activation update for l=L-1, t=1,...,T-1, (line 4 again, check the Indicator functions)\n",
        "        return\n",
        "\n",
        "    def _activation_update_Lminus1_T(self, _arguments_here):\n",
        "        # Implement the Activation update for l=L-1, t=T, (line 7 again, check the Indicator functions)\n",
        "        return\n",
        "\n",
        "    # ============ lagrange multiplier update ============\n",
        "\n",
        "    def _lambda_update(self, arguments_here):\n",
        "        # Implement the update of the lagrange multiplier lambda (Line 15 of Algorithm 2)\n",
        "        return\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        # Implement the forward pass of the SNN.\n",
        "        # It can be implemented using SNNTorch using:\n",
        "        # - The snn.leaky integrate-and-fire neuron model setting the correct arguments as follows:\n",
        "        #   - beta = deltas (the decay factors)\n",
        "        #   - threshold = thetas (the thresholds)\n",
        "        #   - reset_mechanism= 'subtract' for layers l=1,..., L-1, and 'none' for the last layer\n",
        "\n",
        "        # Check SNNTorch tutorials before, if not sure how to implement this\n",
        "        # https://snntorch.readthedocs.io/en/latest/tutorials/index.html\n",
        "\n",
        "        # The outputs should be the membrane potentials of the last layer at time step T\n",
        "        return\n",
        "\n",
        "    def fit(self, _arguments_here):\n",
        "        # This function updates the optimization variables, given an input batch of data samples.\n",
        "\n",
        "        # Carry out the updates following algorithm (2)\n",
        "\n",
        "        # Here is a skeleton of the implementation:\n",
        "        for l in range(1, self.L):\n",
        "            # Update self.W[l] using the function _weight_update\n",
        "            self.W[l] = None\n",
        "            pass\n",
        "            for t in range(1, self.T):\n",
        "                if l < self.L - 1:\n",
        "                    # update self.a[l][t] using _activation_update\n",
        "                    self.a[l][t] = None\n",
        "                else:\n",
        "                    # update self.a[l][t] using _activation_update_Lminus1\n",
        "                    self.a[l][t] = None\n",
        "\n",
        "                # update selfz[l][t] using the function _z_update and check_entries\n",
        "                self.z[l][t] = None\n",
        "                pass\n",
        "\n",
        "            if l < self.L-1:\n",
        "                # update self.a[l][T] using the function _activation_update_T\n",
        "                self.a[l][self.T] = None\n",
        "            else:\n",
        "                # update self.a[l][T] using the function _activation_update_Lminus1_T\n",
        "                self.a[l][self.T] = None\n",
        "\n",
        "            # update self.z[l][T] using the function _z_update_T and check_entries\n",
        "            self.z[l][self.T] = None\n",
        "            pass\n",
        "\n",
        "        # ----- Update the last layer -----\n",
        "        # Update self.W[L] using the function _weight_update_L\n",
        "        self.W[self.L] = None\n",
        "        for t in range(1, self.T):\n",
        "            # update self.z[L][t] using the function _z_update_L\n",
        "            self.z[self.L][t] = None\n",
        "            pass\n",
        "\n",
        "        # ----- Update the last layer at time T -----\n",
        "        # update self.z[L][T] using the function _z_update_L_T\n",
        "        self.z[self.L][self.T] = None\n",
        "\n",
        "        # Update the lagrange multiplier using the function _lambda_update\n",
        "        self.lambda_lagrange = None\n",
        "        return\n",
        "\n",
        "    def evaluate(self, _arguments_here):\n",
        "        # Standard evaluation phase\n",
        "        return\n",
        "\n",
        "    def warming(self, _arguments_here):\n",
        "        # Just as previous implementation.\n",
        "        return\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Python module that includes the N-MNIST dataset\n",
        "    # Check this tutorial for more info: https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_7.html\n",
        "    import tonic\n",
        "\n",
        "    # Implement the training and evaluation of the model, using the N-MNIST dataset\n",
        "    dataset = tonic.datasets.NMNIST(save_to='./data', train=True)\n",
        "\n",
        "    # Implement dataset splitting, training and evaluation below ...\n"
      ]
    }
  ]
}